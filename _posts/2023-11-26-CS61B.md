---
title: CS61B:Data Structures
date: 2023-11-26 14:45:00 +0800
categories: [CourseNotes, CS]
tags: 

comments: false
math: true
mermaid: true
toc: true

pin: true

img_path: /assets/img/CS61B/
---
本文是关于：CS61B的想法、评价、完成情况、自学建议、资源和笔记分享。

> （PC端）如果你没有看到页面右侧的目录，可以尝试调整浏览器缩放比例。\
(On PC)If you don't see the table of contents in the right, try zooming the page.
{: .prompt-warning}

## About CS61B

## Completion & Suggestions

## Resources

# Notes

## Notes: Java Syntax

### Static and Non-Static

### Primitives and Reference Types

### Inheritance

### Etc.

## Notes: Data Structures

### Linked Lists

#### IntList

简单直接的一种基本数据结构。使用者需要知道implementation的细节，直接对原始的结构进行操作，因而容易产生问题，如null-pointer。

#### SLList

> SLList = Singly-Linked List.

- 将IntList包装后，使用者不用再考虑`null`的情况。

```java
public class SLList {

    /* 仍然以IntList（现在叫IntNode）为基础 */
    public class IntNode {
        public int item;
        public IntNode next;

        public IntNode(int i, IntNode n) {
            item = i;
            next = n;
        }
    }

    /* 关键想法：对IntList进行一层包装 */
    public IntNode first;
    public SLList(int x) {
        first = new IntNode(x, null);
    }

    /** Adds an item to the front of the list. */
    public void addFirst(int x) {
        first = new IntNode(x, first);
    }

    /** Adds an item to the front of the list. */
    public void addFirst(int x) {
        first = new IntNode(x, first);
    }
```

- Access control：

1. （对外）将`first`定为`private`，只对外界展示interface，掩蔽implementation细节：
```java
public class SLList {
	private IntNode first;
	...
```
2. （对内）将`IntNode`定为`static`，防止其访问`SLList`的fields（也可以节省一些内存）：
```java
public class SLList {
	public static class IntNode {
	...
```

> If you don't use any instance members of the outer class, make the nested class static. --Josh
{: .prompt-tip}

- `AddLast()`和`size()`:

```java
/** Adds an item to the end of the list. */
public void addLast(int x) {
    IntNode p = first;

    /* Advance p to the end of the list. */
    while (p.next != null) {
        p = p.next;
    }
    p.next = new IntNode(x, null);
}

// size的helper function：
/** Returns the size of the list starting at IntNode p. */
private static int size(IntNode p) { // 留意这里的parameter
    if (p.next == null) {
        return 1;
    }

    return 1 + size(p.next);
}

public int size() {
    return size(first);
}
```

> 虽然两个同名为size的method在同一个class中，只要他们的parameter不同，就是可行的，Java会根据使用时传入的parameter自动选择对应的method。这在Java中叫***Overloading*** 。
{: .prompt-tip}

这个`size()`的效率仍然不高，不过因为我们将IntList的recursive structure进行了包装，我们可以在SLList内通过缓存（*caching*）来获得$O(1)$的runtime：

```java
public class SLList {
    ... /* IntNode declaration omitted. */
    private IntNode first;
    private int size; //

    public SLList(int x) {
        first = new IntNode(x, null);
        size = 1; //
    }

    public void addFirst(int x) {
        first = new IntNode(x, first);
        size += 1; //
    }

    public int size() {
        return size; //
    }
    ...
}
```

> 为什么IntList不能caching呢？设想一下删除中间一个节点。

- 如果我们有一个空的list:

```java
first = null;
size = 0;
```

这会使得`getLast()`报错，因为他会尝试 `first.next`。当然，我们永远可以在每个与空list有关的method中添加判断`if (first == null) ...`，但这会让代码逻辑变得越来越臃肿以至于难以maintain(设想一下，你的数据结构有四五个special case)。

做法是修改`SLList`的结构，添加一个*sentinel node*，我们通过sentinel进入实际上的list。不论list是不是空的，他都在那。

![SLList1](SLList1.png)
_??可以是任何值，他没有实际意义_

SLList相对IntList的改进：
- 内部implementation不暴露于外界;
- `size()`是$O(1)$;
- 可以创建empty list。

#### DLList

> DLList = Doubly-Linked List.

主要是为了提升SLList的效率。一些method如`addLast()`在需要遍历整个list，顺着前面的想法很容易想到增加一个sentinel指向最后一个node，但这并未触及根本：整个list的遍历顺序是单向的，很多情景如`get()`和`removeLast()`并没有得到提升。

我们采取的做法是：改变node的implementation，将其变成双向的。

> 另一种想法会把linked-list变成binary tree。

```java
public class IntNode {
    public IntNode prev;
    public int item;
    public IntNode next;
}
```

到了这一步，自然地，我们也不需要专门指向最后一个的sentinel：可以把结构设计成环形的。

具体implementation是proj 1a的内容，可以看[我的repo](https://github.com/2975459755/CS61B_sp18/blob/main/proj1a/ArrayDeque.java)。

![DLList1](DLList1.png)

### Array List

以Java自带的array为basic implementation来提升运行效率。通过下标操作array的runtime为$O(1)$，这会显著改善`get()`等method的效率。

几点细节：
1. Arraylist的第一个item不需要是array[0]，为了让`removeFirst()`, `addFirst()`等以$O(1)$运行，设置两个pointer分别指向首尾，进行`removeFirst()`时只需要移动第一个pointer。一个写代码的逻辑：只需要满足interface即可，implementation的逻辑未必与表现相同。
2. 因为array list的存储对象可能是reference type，如果`remove()`只是简单地移动pointer，可能会造成本应被garbage collection解决掉的object仍然存在于内存中（因为array里仍然有一个指向它的pointer），因此我们还需要把原来的位置设置为`null`。
3. 创建empty list时，如何设置两个pointer？我的做法是，`first`在`last`后面一位，这样与`add()`，`remove()`等操作的逻辑是连贯的。

- 我们遇到的最主要的问题是resizing：

Java的array是固定大小，当装满以后，我们需要创建一个更大的array，然后把原来的item都copy进去（`System.arraycopy`或从首端pointer开始遍历到尾端pointer），这个操作是$O(n)$。

如果新array的大小是原来大小加上一个固定的值，当$n$足够大，创建一个大小为$n$的list其runtime为$O(n^2)$（对其求导为$O(n)$，即resizing的runtime，因为resizing的频率与$n$成线性关系），这显然是不可接受的。但如果用的array大小远超实际大小，又会造成内存的浪费。

采取的做法是***geometric resizing***：每次需要resizing时，新的array大小为原大小的一个倍数（通常是2倍）。这样resizing的频率与$n$成logarithmic关系。

另外，当我们删除了一定数量的item，使得array过于稀疏时，也需要进行类似的操作。

### Disjoint Sets

什么是Disjoint sets：
1. 每个item被视为在一个set中；
1. 一个item可以与别的item连接；
2. 两个item连接时，对应的两个set中的所有item都互相连接。

```java
public interface DisjointSets {
    /** connects two items P and Q */
    void connect(int p, int q);

    /** checks to see if two items are connected */
    boolean isConnected(int p, int q); 
}
```

基于这个定义，一个直接的implementation是用一个list存储所有的set：`[{0}, {1}, {2}, {3}, {4}, {5}, {6}]`，但这个想法的实现实际非常复杂低效：设想一下要`connect(4, 5)`，需要先遍历找到4和5。

#### Quick Find

只用一个array来存储所有item：

![djs1](djs1.png)
_{0, 1, 2, 4}, {3, 5}, {6}_

下标代表具体的item，而value对应item所在的set。

当进行`connect(2, 3)`以后，所有set数值为4(即`array[2]`)的item的value都被改为5，这个过程最坏的情况是$O(n)$。

![djs2](djs2.png)

`isConnected(p, q)`需要确认`array[p] == array[q]`，这个过程则只需要$O(1)$，因此这被叫做*Quick Find*。

```java
public class QuickFindDS implements DisjointSets {

    private int[] id;

    /* Θ(N) */
    public QuickFindDS(int N){
        id = new int[N];
        for (int i = 0; i < N; i++){
            id[i] = i;
        }
    }

    /* need to iterate through the array => Θ(N) */
    public void connect(int p, int q){
        int pid = id[p];
        int qid = id[q];
        for (int i = 0; i < id.length; i++){
            if (id[i] == pid){
                id[i] = qid;
            }
        }
    }

    /* Θ(1) */
    public boolean isConnected(int p, int q){
        return (id[p] == id[q]);
    }
}
```

#### Quick Union

仍然沿用前面的notation，用同一个array的index和value分别表示item和其所在的set，但**这个array同时用来表示tree**：

![djs3](djs3.png)

每一个tree对应一个set，每个item视为tree的node。我们需要一个`find(item)`来找到item的root：`array[item]`返回item的parent，那么`find(item)`将不断iterate（或者用recursion）直到找到root。这里将root的值定为-1，用来标识。

*理想地*，`find()`最坏情况是$O(logN)$，即tree的最大高度。

`connect(p, q)`使用`find(p)`找到`p`的root，然后将root的parent设为另一个tree的root，即`find(q)`。

```java
public class QuickUnionDS implements DisjointSets {
    private int[] parent;

    public QuickUnionDS(int num) {
        parent = new int[num];
        for (int i = 0; i < num; i++) {
            parent[i] = i;
        }
    }

    private int find(int p) {
        while (parent[p] >= 0) {
            p = parent[p];
        }
        return p;
    }

    @Override
    public void connect(int p, int q) {
        int i = find(p);
        int j= find(q);
        parent[i] = j;
    }

    @Override
    public boolean isConnected(int p, int q) {
        return find(p) == find(q);
    }
}
```

*理想地*，`connect()`的最坏情况也是$O(logN)$，但这取决于不断连接之后tree的形状是否保持良好(bushy / spindly)，这将是quick union讨论的重点。就目前的算法而言，我们的tree完全有可能退化成linked list（不断将tree粘到只有一个node的tree上），那么`find()`和`connect()`的最坏情况将是$O(N)$。

#### Improving Quick Union

***Wighted Quick Union***: 永远将更小的tree粘到更大的tree的root上。

![djs4](djs4.png)
_option 2更理想_

因此我们需要知道每个tree（set）的具体大小（weight），可以将`array[root]`设为`-(size)`。

如此一来tree的最大高度为$\log_{2}{N}$，我们便获得了$O(logN)$的`connect()`和`isConnecteed()`。

***Path Compression***: 基于每次`connect()`和`isConnected()`都会call `find()`来找到item的root，可以在`find()`以后直接将item的parent设为root，这样我们会得到$O(1)$的[amortized
 runtime](https://joshhug.gitbooks.io/hug61b/content/chap8/chap84.html#amortized)。

#### Summary

|Implementation|Constructor|`connect`|`isConnected`|
|Quick Find|$\Theta(N)$|$\Theta(N)$|$O(1)$|
|Quick Union|$\Theta(N)$|$O(N)$|$O(N)$|
|Weighted QU|$\Theta(N)$|$O(logN)$|$O(logN)$|
|WQU with Path Compression|$\Theta(N)$|$O(\alpha(N))^*$|$O(\alpha(N))^*$|

> $\alpha(N)$: Behaves as constant in long term.

### BST

在Binary Tree的基础上，规定左右秩序的tree。

> *非正式的形象理解*：可以将BST视为doubly-linked list的一种延伸：在sorted list中，不再线性的从头到尾看待，而是将中间某个node视为第一个，即root（理想地这个node刚好处在最中间）；对于其左边和右边，也采取同样的操作。视觉上，你可以想象原本排成一行的list从中间被拎起，被拎起的节点又带着两边各自中间的节点...如此便拎起来一棵树。

#### Insert

我们的BST不允许有重复，如果`insert`已有的值，BST不会发生任何变化：

```java
static BST insert(BST T, Key ik) {
  if (T == null)
    return new BST(ik);
  if (ik < T.key)
    T.left = insert(T.left, ik);
  else if (ik > T.key)
    T.right = insert(T.right, ik);
  return T;
}
```

需要注意`if (T == null)`，我们*不需要*分别判断`left`或`right`是`null`，然后赋值，而是往那个方向继续recursion，然后`return new BST(ik)`。***这是由recursion的写法决定的***，请确保你真正明白了其中的逻辑。

> 随着不断地`insert`，如果root恰好在该序列中处于偏远的位置，BST将会变得spindly。
{: .prompt-tip}

#### Delete

三种情况：
1. Node没有child：将parent的`left`或`right`设为`null`；
2. Node只有一个child：将parent的`left`或`right`设为该child；
3. Node有两个child：不能简单的让其中一个代替原来的位置再把另一个接在下面，这不保证仍然会是BST。我们用*Hibbard Deletion*：

基于BST的性质，`left`的最右端或`right`的最左端在值上最接近root，因此我们将其中一个移到root的位置来取代要被删除的值。

![bst1](bst1.png)
_紫色的cat或elf将是root的继承者_

Implementation: 与`insert`类似。

### Balanced Trees

在BST中提到，在一系列操作后如果root在序列中偏离中点较远，BST将不能维持良好的形状，接下来将探讨几种维持bushiness的方法（有可能会改变tree本身的结构）。

#### B-Tree / 2-3 Tree / 2-3-4 Tree

一个极端的想法：如果不创建新的node，那么我们的树将永远维持最佳的形状。考虑永远将新的item加入已有的node里（注意这里已经不是BST），可以想到这个方案的实现比bst难得多，且本质上并不会更好，

![balanced2](balanced2.png)

一直insert更大的item，仍然会得到$O(N)$的runtime。

那么，设定一个node可以囊括item的最大数量，比如3；当第4个item加入时，将这个node中间(偏左)的item***上移***到parent中，（如果没有parent，他将成为新的root）

![balanced3](balanced3.png)
_第一步：上移中间的item_

这带来新的问题，原本比被移上去的item更小的（16）实际上排在parent和上移的item中间，因此***将它变成新的中间节点***，

![balanced4](balanced4.png)
_第二步：产生新的分支_

B-Tree*总是*维持$logN$的高度。

#### Rotating BST

```java
private Node rotateRight(Node h) {
    // assert (h != null) && isRed(h.left);
    Node x = h.left;
    h.left = x.right;
    x.right = h;
    return x;
}

// make a right-leaning link lean to the left
private Node rotateLeft(Node h) {
    // assert (h != null) && isRed(h.right);
    Node x = h.right;
    h.right = x.left;
    x.left = h;
    return x;
}
```

图像可以参考下面[Red-Black Tree](#red-black-tree)中rotation的图，忽略其中的红/黑色即可。

#### Red-Black Tree

BST很容易实现，但有着易失衡的天然缺陷；B-Tree能够维持良好的平衡，但具体实现非常复杂。Red-Black Tree则结合了两者的优点：用BST表示B-Tree。

我们的Red-Black Tree与B-Tree是*一一对应*的，要实现前者，只需要弄清楚对应关系。

以2-3 tree为例：

> 这部分因为作业没有涉及，暂时还没搞明白。
{: .prompt-warning}

- **如何表示包含多个item的node？**

![balanced5](balanced5.png)

定义red-black tree为*有两种不同link*的BST，我们用红色和黑色区分。Red link表示B-Tree单个node内部的连接，所有red link必须是相同方向的（为什么？）。61B和Algs4选择red link向左下，即left-leaning. 

与2-3 tree等价的定义：
1. Red link朝向左下；
2. 一个node不能同时拥有两个red link；
3. Red link不计入高度：tree的平衡性计算只考虑root到null link的black link的数量；
4. 不能有两条连续的（向左的）red link。

![llrb1](LLRB1.png)

- **如何表示red/black link?**

在node class中增加一个instance variable，表示*指向该node的link*的颜色。因为只有两种，可以用Boolean来节省空间。

```java
private class Node {
	Key key;          // key
	Value val;        // associated data
	Node left, right; // subtrees
	int N;            // # nodes in this subtree
	boolean color;    // color of link from
	// parent to this node
	Node(Key key, Value val, int N, boolean color) {
		this.key = key;
		this.val = val;
		this.N = N;
		this.color = color;
	}
}
private boolean isRed(Node x) {
	if (x == null) return false;
	return x.color == RED;
}
```

- **Rotation**

当出现向右的red link或是两条连续的Red link时，我们用rotation来维持与2-3 tree的对应关系。其逻辑与普通BST的rotation相同，只是*要同时改变link的颜色*。

![llrb2](LLRB2.png)

```java
Node rotateLeft(Node h) {
	// rotate
	Node x = h.right;
	h.right = x.left;
	x.left = h;
	// 交换颜色
	x.color = h.color;
	h.color = RED;
	// 维持对size的计数
	x.N = h.N;
	h.N = 1 + size(h.left)
	+ size(h.right);

	return x;
}

Node rotateRight(Node h) {
	// 同理
}
```

- **Put / Insert**

在B-Tree中，`put`永远是先将item加入已有的node中。对应地，*新加入的link永远是红色的*。当然，这里有几种情况需要分开讨论：

1. 新加入的red link是朝右的：只允许向左的red link，需要rotation（在left-leaning的前提下，`rotateLeft`）；
2. 有两条连续的red link：对应2-3 tree的同时有三个item的node；需要rotation（这里是`rotateRight`）；
3. 一个node同时有向左和向右的红色link：对应B-Tree的分支操作，将此与node连接的*所有*link都变色。

![llrb3](LLRB3.png){: .w-50 .right}
对于后两条的解释：当2-3 tree有node容纳3个item时，我们将中间的item上移，剩余两个item分别变成单独的node。对应到这里，当中间node已经（通过black link）直接连接parent时（这时连接他的两条red link分别是左下和右下），只需要改变连接他的所有link的颜色即可（你可能会想如果这个node在parent的右边怎么办，那是第一条的作用）；当中间node没有直接连接parent，对其`rotateRight`，这样就是上面的情况了。

想明白这些以后，自然的会想到`put`的recursive structure大致是什么样子：先`compare`，直到找到插入的点，然后向上一层一层调整。

```java
private Node put(Node h, Key key, Value val) {
    if (h == null) { return new Node(key, val, RED); }

    int cmp = key.compareTo(h.key);
    if (cmp < 0)      { h.left  = put(h.left,  key, val); }
    else if (cmp > 0) { h.right = put(h.right, key, val); }
    else              { h.val   = val;                    }

    if (isRed(h.right) && !isRed(h.left))      { h = rotateLeft(h);  } // 情况1
    if (isRed(h.left)  &&  isRed(h.left.left)) { h = rotateRight(h); } // 情况2
    if (isRed(h.left)  &&  isRed(h.right))     { flipColors(h);      }  // 情况3

    return h;
}
```

- **Delete**

这部分课上完全没有讲，或许我看了Algs4之后会回来补充这部分<del>如果我还记得</del>。

### Tree Traversal

### Hashing

### Heaps, PQ

#### Priority Queue

首先定义Priority Queue：

> 课上选择的是minPQ，不过我为了方便用algs4的图就用了maxPQ。

```java
/** (Max) Priority Queue: Allowing tracking and removal of 
  * the largest item in a priority queue. */
public interface MaxPQ<Item> {
    /** Adds the item to the priority queue. */
    public void add(Item x);
    /** Returns the largest item in the priority queue. */
    public Item getMax();
    /** Removes the largest item from the priority queue. */
    public Item removeMax();
    /** Returns the size of the priority queue. */
    public int size();
}
```

对比不同structure的runtime：

|Data Structure|add|get|remove|
|Ordered Array|$\Theta(N)$|$\Theta(1)$|$\Theta(N)$|
|Unordered Array|$\Theta(1)$|$\Theta(N)$|$\Theta(N)$|
|BST(Optimal)|$\Theta(\log{N})$|$\Theta(\log{N})$|$\Theta(\log{N})$|
|HashTable|$\Theta(1)$|$\Theta(N)$|$\Theta(N)$|
|***Heap***|$\Theta(\log{N})$|$\Theta(1)$|$\Theta(\log{N})$|

#### Heap: Definition, Representation

> 在这里我们默认heap = BINARY heap

对于binary tree，施加一些限制使其在满足我们的需求前提下最优化performance。

定义Heap：
1. Priority(parent node) >= Priority(child);（不是正式的代码）
2. Complete binary tree;

对于任何tree，*complete*的含义是从上到下、从左到右没有空位。借助这一点，我们可以用array来表示这种tree：

![heap1](heap1.png)

注意：我们忽略array的第一项，以获取下标运算的简洁性。

```java
/**
 * Returns the index of the node to the left of the node at i.
 */
static int leftIndex(int i) {
    return 2 * i;
}

static int rightIndex(int i) {
    return 2 * i + 1;
}

static int parentIndex(int i) {
    return i / 2;
}
```

#### Heap Operations

在一些情况下，我们需要对heap进行操作来维持其特性：
1. 删除root；
2. 某一item的priority改变；
3. 添加item；

heap有一对维持特性的基本操作，分别是`swim` (*bottom-up reheapify*) 和`sink` (*top-down reheapify*)。

```java
/**
 * Bubbles up the node currently at the given index.
 */
void swim(int index) {
    if (index <= 1) return;

    int parent = parentIndex(index);
    if (min(index, parent) == index) {
        swap(index, parent);
        swim(parent);
    }
}

/**
 * Bubbles down the node currently at the given index.
 */
void sink(int index) {
    if (index >= size) return;

    int left = leftIndex(index);
    int right = rightIndex(index);
    int min = min(left, right); // Be sure to swap with the minimum child!
    if (min(index, min) != index) {
        swap(index, min);
        sink(min);
    }
}
```

![heap2](heap2.png)

在此基础上，其余操作分别是：
- Delete：将root与array中最后一个item交换，删除原来的root，然后对新的root进行`sink`；
- Insert：将新的item添加在array末尾（要记得heap是complete tree），然后对其进行`swim`；
- ChangePriority：改变该item的priority后，对其进行`swim`和`sink`。

![heap3](heap3.png)

还有一些细枝末节，如resizing array，validating arguments等等，就略去了。

因为heap在维持最值上的特性，我们还会在heap sort中用到它。

### Graphs

### Graph Traversal

## Notes: Sorting Algorithms

### Selection Sort

将一个array分两部分看：Sorted 和 Unsorted，最初整个array都是unsorted。每次循环从unsorted中找到最值，与第一个unsorted item交换，由此sorted部分长度+1。重复直到只有一个unsorted item。

```java
static void seletionSort(Comparable[] a) { // Sort a[] into increasing order.
	int N = a.length; // array length
	for (int i = 0; i < N; i++) { // Exchange a[i] with smallest entry in a[i+1...N).
		int min = i; // index of minimal entr.
		for (int j = i+1; j < N; j++) { // 找到unsorted array的最值
			if (less(a[j], a[min])) min = j;
		}
		exch(a, i, min); // 交换
	}
}
```

Seletion sort的worst case runtime为 $n + (n - 1) + \cdots + 1 = n^2 / 2$ 次compare和 $n$ 次exchange，即$O(n^2)$。

特性(algs4 P248)：
1. Running time is insensitive to input.
2. Data movement is minimal.

### Insertion Sort

![insertion sort](insertion.png)

同为$O(n^2)$，Insertion sort比selection sort往往要快；

Insertion Sort在以下两种情况表现很好，因而在实际应用中，被作为Quick Sort等算法在特殊情况下的补充：
1. Partially sorted;
2. Tiny arrays;

### Heap Sort

确切的讲，我们使用的是In-place Heap Sort：直接将原array改成heap。以升序排序为例，用maxHeap，每次deleteMax之后heap原来最后的位置成为空缺，将该max放入。如此，在heap的右侧我们将会有一个升序序列，并且他们的位置就是最终的位置。

在这种情况下为什么不用minHeap？

> 因为array representation的heap总是从位置0开始，在右端缩减。deleteMin之后只能放入右端，而那不会是最终位置：再次deleteMin，需要将右边的sorted array整体前移一格。因此minHeap可行，但单纯地更麻烦，性能更差。

- Heap Construction：Algs4上说从前到后bottom-up-heapification的效率不如从后往前的top-down-heapification，不过这点我还没搞明白；

```java
public static void sort(Comparable[] a) {
	int N = a.length;
	for (int k = N/2; k >= 1; k--) 
		sink(a, k, N);
	while (N > 1) {
		exch(a, 1, N--);
		sink(a, 1, N);
	}
}
```

时间复杂度：
- Heapification: $O(N\log N)$. 

> Algs4 P323: Sink-based heap construction uses fewer than 2N compares and 
fewer than N exchanges to construct a heap from N items.

- SortDown: $O(N\log N)$.

> Algs4 P326: Heapsort uses fewer than 2N lg N + 2N
compares (and half that many exchanges) to sort N items.（注：$2N$来自第一步）

空间复杂度：$\Theta(1)$。

### Merge Sort

Merging：将两个已经有序的序列合并成一个有序序列，如下是一种思路：

```java
public static void merge(Comparable[] a, int lo, int mid, int hi)  { 
	// Merge a[lo..mid] with a[mid+1..hi].
	int i = lo, j = mid+1;
	for (int k = lo; k <= hi; k++) // Copy a[lo..hi] to aux[lo..hi].
		aux[k] = a[k]; // aux 是 auxiliary 的简写
	for (int k = lo; k <= hi; k++) { // Merge back to a[lo..hi].
		if (i > mid) a[k] = aux[j++];
		else if (j > hi ) a[k] = aux[i++];
		else if (less(aux[j], aux[i])) a[k] = aux[j++];
		else a[k] = aux[i++];
	}
}
```

想法：对想要排序的序列，分成两半，对其排序后进行merging；如何排序？答案：Recursion。

从大方向看，merge sort有两种写法：Top-down（一分为二）和Bottom-up（二合为一）。

> 顺带一提：61B课上讲的是前者，lab里要求做的是后者，但没有直接告诉你思路，只告诉你各个subroutine的作用，如果先前没有了解过，要想出这种做法还是要动点脑筋的。

- 时间复杂度分析：考虑对长度为N的序列需要做的对比操作次数$C(N)$，给出其upper-bound：

$$C(N) \le C(\lfloor N/2 \rfloor) + C(\lceil N/2 \rceil) + N$$

其中N来自merge操作，其近似解（对于 $N = \exp 2$ 这是准确解）：

$$C(N) = N\log N$$

以感性的认识也很容易得出这个结论。

我特地写了一点过程而在别的算法中直接给出结果，因为我感觉这个模式在很多算法中都能见到：除了merge sort，还有quick sort，FFT等。

- 空间复杂度(top-down)：$\Theta(N)$

对于linked-list形式的数据，bottom-up更为有效，因为可以in-place。我们在lab中写的merge sort就是这种情况。

### Quick Sort

Quick sort像是merge sort的反面：后者先divide，再对整个array进行操作（merge），而前者先对整个array进行操作（pivoting）后，再divide。

Quick sort的基本逻辑：在每一轮recursion中，选择一个item作为pivot，将其移动到最终位置（即，一侧都不比他大，另一侧都不比他小），然后对两侧进行同样的操作。

...

## Notes: Etcetera

### Invariants

创造或寻找代码中的invariant，会使得代码逻辑更清晰。

如我们在[SLList](#sllist)中创建*sentinel node*，避免考虑过多corner case：

![invariant1](invariant1.png)

### Interface vs Implementation

Interface只规定了代码对外界的表现，具体implementation如何、是否与表现拥有相同的逻辑则完全**独立于**interface，你可能会记得[ArrayList](#array-list)课上Josh放的这张图：

![interface1](interface1.png)

我们使用Java的array作为list的implementation，但`list.get(0)`实则未必与`array[0]`相同，甚至`remove()`之后被'删除'的item还仍可能存在于array中（虽然通常我们还是把保存在array中的reference删除掉）。

