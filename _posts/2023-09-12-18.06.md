---
title: 18.06:Introduction to Linear Algebra
date: 2023-09-12 19:24:00 +0800
categories: [CourseNotes, Math]
tags:

comments: false
math: true
mermaid: true
toc: true

pin: true

img_path: /assets/img/18.06/
image: head.png
---
本文是关于：18.06的想法、评价、完成情况、自学建议、资源和笔记分享。

> （PC端）如果你没有看到页面右侧的目录，可以尝试调整浏览器缩放比例。\
(On PC)If you don't see the table of contents in the right, try zooming the page.
{: .prompt-warning}

## About 18.06

18.06是MIT面向非数学专业的线代课（虽然也有一些不从analysis开始的数学专业指南推荐这门课或课本）。不同于一些名字带着introduction实则硬核至极的课，18.06的内容并没有多少难度 --- 即使你只看课程视频，做课后给的两三道题，也足够跟上课程的节奏 。但正如Strang所说："But it's nice to see the things come out right"，挑战性的缺失并没有让这门课变得无趣，相反，我相信当你在学习过程中反复看到本文开头的图像，对其有崭新的认识以后，你也会认同这个观点。

![Strang](strang.png)
_Strang的最后一节课(May 15, 2023)_

课程主讲Gilbert Strang教授已经讲授18.06数十年，本课程视频是在千禧年前夕录制，当时他已经年逾古稀，前不久89岁的老教授讲授了他的最后一课，结束了62年的教学生涯。[MIT news](https://news.mit.edu/2023/gilbert-strang-made-linear-algebra-fun-0531)评价：“He made linear algebra fun"。

另外Strang还教授两门后续课程，是关于applied linear algebra： 18.065 和 18.085、18.086。

## Completion & Suggestions

|估计用时|200H(<80H)|看课本,做笔记和做课本的题花了很多时间，括号内是除开这几项的时间|
|Lecture videos|$\checkmark$||
|Lecture summary|$\checkmark$||
|Recitation|$\checkmark$|有人带着做题的机会还是挺宝贵的|
|Problem set|$\checkmark$|一节课就两三题，也不难，就别挑了|
|Textbook|$\checkmark$|书比较wordy，看过课的话建议跳读|
|PSet(textbook)|$\checkmark$|做到第7章前。难度不大题量大，建议挑一部分做|

- lecture, summary, textbook的区别：summary不一定按照课上讲的顺序来，与其说是summary，更像是reflection；textbook比视频涉及的东西没有多很多，但写的很细，所以很长。
- 接上条：建议以视频为主，简单过下summary，不清楚的地方针对地看textbook。
- 关于学习顺序：18.06由求解linear system of equations引入matrix和vector space，最后再讲linear transformation。而传统的顺序是由linear transformation引入，虽然Strang提到过是有意为之，我还是认为后者的角度切入会更自然，而且会帮助对后面的multiplication, inverse, projection, diagonalization等话题有更形象的理解。因此我***建议***搭配3B1B的视频（见resources），对linear transformation有直观的认识后，额外带着这个角度去思考。
- 接上条：我认为gatech的课本的顺序安排的很好，但内容要少一些。
- Textbook的习题中概念和基本的计算较多。建议跳着做，因为真的很多。

## Resources

- [课程主页(OCW)](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/)

> 未特殊说明的资源均可在主页上直接获取。
{: .prompt-info}

|Lecture videos|$\checkmark$||
|Lecture summary|$\checkmark$||
|Recitation videos|$\checkmark$|With transcripts|
|Problem set|$\checkmark$|With solutions|
|Textbook|$\checkmark$|见下|
|Textbook solutions|$\checkmark$|见下|

- 课本(点击下载)：[Introduction to Linear Algebra (5th Edition)](/assets/img/18.06/introduction-to-linear-algebra-fifth-edition-5nbsped-0980232775-9780980232776_compress.pdf)\
最新版是第六版。这本书是没有官方pdf的，清华有做第五版的影印版：《线性代数》。\
课本像是为自学者设计的，梯度平缓并且有很多具体的例子。有一点基础的话直接看下面那本会更好。
- 课本习题答案(官方)(点击下载)：[Solution Manual](https://math.mit.edu/~gs/linearalgebra/ila5/ila5sols.pdf)\
其实这个在主页上有，只是不显眼。
- Gatech的课本(链接至网页)：[Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/index.html)
- 3blue1brown的线代短课程(链接至网页)，可以帮助建立intuition：[Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- 一本很不错的书，个人觉得更适合作为听过课的基础上使用的课本(点击下载)：[David.C.Lay: Linear Algebra And Its Applications](/assets/img/18.06/Linear-Algebra-And-Its-Applications.pdf)

## $\downarrow\downarrow$ Notes

- 这是我第一份'有很多字的'笔记，是在学习过程中记的。主要目的之一是“当我忘记具体的知识细节的时候可以通过这份笔记快速了解其来龙去脉”，因此会显得不够精简。但如果你想把它作为看课的辅助，我觉得还是不错的，不过需要注意*这份笔记不完全按照课程的顺序记录*。

笔记很长，做的时候还要反复翻书整理，加之这是我第一次用latex写公式，零零总总这份笔记加在一起可能花了我三十小时的时间，这是我未曾想到的。

因为已经习惯英文的输入，当我尝试用中文写的时候发现反而不习惯<del>（而且不方便我抄书）</del>，就用英文写了。顺便想吐槽一下中文一些术语的翻译，比如学之前我看过国内某教材目录，主要讲行列式和特征值，使我学的时候一度以为行列式是矩阵的另一种叫法，而determinant是特征值...

- 内容主要分为两大部分，每一部分都以能综合运用所学知识的应用数学方法来结尾：
1. [第一部分](#0-introduction-linear-systems): 求解$Ax = b$，vector space，orthogonality。以[least squares](#least-squares-ax-neq-b)收尾。
2. [第二部分](#4-determinants): 从eigenvector的视角研究matrix。以[SVD和pseudoinverse](#svd-a--usigma-vt)收尾。

- [Linear transformation](#diagonalization-similarity)同样精彩，我对这个章节进行了补充（课和课本对这块涉及较少）。我们将会发现similar matrices是不同坐标体系下相同的transformation。

# First Half: Ax = b

### 0. Introduction: Linear Systems

Start with systems of linear equations, for example: 

$$ ax_{1} + bx_{2} = b_{1}$$

$$ cx_{1} + dx_{2} = b_{2}$$

We view them in 3 ways:

##### row picture

Each row plots a set of points(i.e. a line) in the 2-dim Euclidean space that satisfies the equation. The solution to this system is the _intersection_ of the two lines.

##### column picture

Consider the system as a single equation --the `linear combination` of the coefficients as vectors:

$$ x_{1}\begin{bmatrix} a \\ c \end{bmatrix} + x_{2}\begin{bmatrix} b \\ d \end{bmatrix} = \begin{bmatrix} b_{1} \\ b_{2} \end{bmatrix}$$

>__Linear Combination__:\
Vector arithmetics combining _addition_ with _scalar multiplicatoin_. Like _c**v** + d**w**_ where _c_, _v_ are the _scalars_ . 
{: .prompt-info}
Now do vector calculation: find the combination of the left side vectors that gives the right side.

##### matrix picture

Rewrite the combination in matrix form:

$$ \begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} = \begin{bmatrix} b_{1} \\ b_{2} \end{bmatrix} $$

The left 2 by 2 matrix A is the _coefficient matrix_, the vector(or column matrix) $\textbf{x} = \begin{bmatrix} x_{1} \\\ x_{2}\end{bmatrix}$ is the unkowns. Values on the right hand side give __b__.

So we have $$ A\textbf{x} = \textbf{b} $$.

## 1. Rules of Matrix Operations

### Addition

- Rectangular matrices can be added if they are of the same shape.
- Additions go one entry at a time--like vector addtitions.
- Commutative law, distributive law and associative law are obeyed.

### Multiplication

- To multipy $AB$: $col(A) = row(B)$
- m by n times n by p gives m by p.
- Associative Law: $(AB)C = A(BC)$
- Distributive Law: $A(B + C) = AB$
- __Commutative Law is NOT allowed!__: $AB \neq BA$

#### rows and columns

-  Matrix A times every column of B:\
	$$A\begin{bmatrix} \mathbf{b_{1}} & \cdots & \mathbf{b_{p}} \end{bmatrix} = \begin{bmatrix} A\mathbf{b_{1}} & \cdots & A\mathbf{b_{p}}\end{bmatrix}$$\
	So each column of AB is a linear combination of columns of A.
-  Every row of A times matrix B:\
	$$\begin{bmatrix} \mathbf{a_{1}} \\ \vdots \\ \mathbf{a_{p}} \end{bmatrix}B = \begin{bmatrix} \mathbf{a_{1}}B \\ \vdots \\ \mathbf{a_{p}B}\end{bmatrix}$$\
	So each row of AB is a linear combination of rows of B.

#### dot products

From the row and column rules we see that (row i, column j) of the product is contributed from (row i of A) and (column j of B). It's not hard to derive:

> $(AB)_{ij} = (\text{row} i \text{ of } A)(\text{col} j \text{ of } B)$
{: .prompt-tip}

#### columns mutiply rows

Multiply column i to n of A and row i to n of B. Add those matrices.

$$ \begin{bmatrix}\mathbf{col}1 & \cdots & \mathbf{col}n\end{bmatrix} 
\begin{bmatrix}\mathbf{row}1 \\ \vdots \\ \mathbf{row}n\end{bmatrix} 
= (\mathbf{col}1)(\mathbf{row}1) + \cdots + (\mathbf{col}n)(\mathbf{row}n)$$

#### block mutiplication

If blocks of A can multiply blocks of B, then block multiplication of AB is allowed. Cuts between columns of A match cuts between rows of B.


$$\begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22}\end{bmatrix}\begin{bmatrix} B_{11} \\ B_{21}\end{bmatrix} = \begin{bmatrix} A_{11}B_{11} + A_{12}B_{21} \\ A_{21}B_{11} + A_{22}B_{21}\end{bmatrix}$$

This is as if the blocks were numbers(1 by 1 blocks).
> _A special case:_ columns time rows follows the same rule. In this case blocks are just columns and rows.

### Exponents(Square Matrices)

- $A^p = AAA\cdots A \text{ (p factors)}$
- $(A^p)(A^q) = A^{p+q}$
- $(A^p)^{q} = A^{pq}$

### Transposes

> Transposes exchange rows and columns:
$$(A^{T})_{ij} = A_{ji}$$
{: .prompt-info}

The matrix 'flips over' its main diagonal.

> Product: $$(AB)^{T} = B^{T}A^{T}$$
{: .prompt-info}

Rows of $A$ are multiplying columns of $B$. Now rows are columns and columns are rows!

> Transpose of inverse: $$(A^{-1})^{T} = (A^{T})^{-1}$$
{: .prompt-tip}

Proof: $A^{-1}A = I$ is transposed to $A^{T}(A^{-1})^{T} = I$. Similarly $AA^{-1} = I$ leads to $(A^{-1})^{T}A^{T} = I$. So $(A^{-1})^{T}$ is the inverse of $A^{T}$.

#### The Meaning of Inner Products

> Make sure to get back to this after exposure to [Projection](#projections) and [Linear Transformation](#6-linear-transformations).

> $^{T}$ is inside: _**The dot product or inner product is $x^{T}y$**_. $(1 \times n)(n \times 1)$\
$^{T}$ is outside: _**The rank one product or outer product is $xy^{T}$**_. $(n \times 1)(1 \times n)$
{: .prompt-info}

Now we have a better way to approach transpose than 'flipping the matrix':

$(Ax)^{T}y = x^{T}(A^{T}y)$:\
Inner product of $Ax$ with $y$ $=$ Inner product of $x$ with $A^{T}y$

$A^{T}$ is the matrix that makes these two inner products equal for every $x$ and $y$.

> Example:\
$$A = \begin{bmatrix} -1 & 1 & 0 \\ 0 &-1 & 1\end{bmatrix}$$ and $$\mathbf{x} = \begin{bmatrix} x_{1}\\x_{2}\\x_{3}\end{bmatrix}$$ and $$\mathbf{y} = \begin{bmatrix}y_{1}\\y_{2}\end{bmatrix}$$.\
On one side we have $$A\mathbf{x}$$ mutiplying $$\mathbf{y}$$ : $$(x_{2} - x_{1})y_{1} + (x_{3} - x{2})y_{2}$$\
That is also $$x_{1}(-y_{1}) + x_{2}(y_{1} - y_{2}) + x_{3}(y_{2})$$, which is $$\mathbf{x}$$ mutiplying $$A^{T}\mathbf{y}$$.\
So $$A^{T}\mathbf{y}$$ must be $$\begin{bmatrix}-y_{1}\\y_{1} - y_{2}\\y_{2}\end{bmatrix}$$ which produces $$A^{T} = \begin{bmatrix} -1 & 0 \\ 1 & -1\\ 0 & 1\end{bmatrix}$$ as expected.

## 2. Elimination: A Systemetic Way of Solving Ax = b

### The Idea of Elimination

The idea is simple: to produce **upper triangular** systems, where at the bottom you directly get the value of one unknown, then substitute upwards, which is called `back substitution`. It's a self-recursive procedure.

$$\text{Before:}\hspace{0.5cm} \begin{align}\begin{split} 
	x - 2y &= 1 \\
	3x + 2y &= 11\end{split}
\hspace{1cm}\text{After:}\hspace{0.5cm}
\begin{split}x - 2y & = 1 \\
	8y &= 8\end{split}\end{align}$$

### Elimination

#### Steps of elimination

Find the _**nonezero**_ entry in (1, 1), which is the `pivot`. 

> Here we assume the pivots could always be found without extra operations. 

Next, eliminate down the pivot: for each row down the pivot row, find the `multiplier` $l_{i1} = A_{i1} / A_{11}$. Substract the multiplier times row 1 from each row, to get all 0's down the pivot. _We say the first column is eliminated_.


Repeat the above steps for pivot(2, 2), so on and so forth. This ends up with all 0's down the main diagonal, producing the upper triangular matrix $U$. 

#### The augmented matrix

Elimination does the same row operations to A and to **b**. Thus we can include **b** as an extra column and carry on elimination.

$$\text{Augmented matrix}\hspace{1cm}\left[\begin{array}{c | c} \large{A} & {\large{b}}\end{array}\right]$$

#### Elimination matrix $E$

Matrix multiplication does effect. We want to find what matrix $E$ is multiplied to eliminate $A$ to produce $EA = U$.

First find the matrix $E_{ij}$ that does one elimination step(eliminate one entry), which is easy to find. Take a 3 by 3 example:

$$E_{31} = 
\begin{bmatrix} 1 & 0 &0\\
	0 & 1 & 0\\
	-l_{31} & 0 & 1\end{bmatrix}$$

The matrix $E_{31}$ has $-l_{31}$ in (3, 1).

Multiply all these matrices $E_{ij}$ to find:

$$E = E_{32}E_{31}E_{21} = 
	\begin{bmatrix} 1 & 0 & 0\\0 & 1 & 0\\0 & -l_{32} & 1\end{bmatrix}
	\begin{bmatrix} 1 & 0 &0\\0 & 1 & 0\\-l_{22}& 0 & 1\end{bmatrix}
	\begin{bmatrix} 1 & 0 &0\\-l_{21} & 1 & 0\\0 & 0 & 1\end{bmatrix} =
	\begin{bmatrix} 1 & 0 & 0\\-l_{21} & 1 & 0\\-l_{22} + l_{32}l_{21} & -l_{32} & 1\end{bmatrix}
	$$

> Notice how the (3, 1) entry of $E$ is not $-l_{31}$. \
Later when we multiply by $E^{-1}$ to get $A = LU$(the LU decomposition), the multipliers will just sit where they should be.

#### Elimination by blocks

> Suppose A is 3 by 3, to eliminate down the first pivot(1, 1), we need:\
$$E_{21} = \begin{bmatrix}
I & \begin{matrix} 0 & 0 \end{matrix}\\
\begin{matrix} -l_{21} \\ 0 \end{matrix} & I
\end{bmatrix} $$ and 
$$E_{31} = \begin{bmatrix}
I & \begin{matrix} 0 & 0 \end{matrix}\\
\begin{matrix} 0 \\ -l_{31} \end{matrix} & I
\end{bmatrix} $$.\
Using inverse matrices, we can do block elimination on a whole column.\
$$E = \begin{bmatrix}
I & \begin{matrix} 0 & 0 \end{matrix}\\
\begin{matrix} -l_{21} \\ -l_{31} \end{matrix} & I
\end{bmatrix}
$$

Block elimination:

$$\left[\begin{array}{c | c} I & \mathbf{0} \\ \hline -CA^{-1} & I\end{array}\right]
\left[\begin{array}{c | c} A & B \\ \hline C & D \end{array}\right] = 
\left[\begin{array}{c | c} A & B \\ \hline \mathbf{0} & \mathbf{D - CA^{-1}B} \end{array}\right]$$

The pivot block is A. The final block is $D - CA_{-1}B$, just like $d - cb/a$. This is the _Schur complement_.

#### Permutation matrix $P$

When we get a 'zero pivot' during elimination, we do row exchange, which is called `permuation`, to resolve the problem.

> **Permutation** 
 $$PA = \begin{bmatrix}
	1 & & \\
	& & 1 \\
	& 1 &\end{bmatrix}
	\begin{bmatrix} row1 \\ row2 \\ row3\end{bmatrix}
	= \begin{bmatrix} row1 \\ row3 \\row2\end{bmatrix}$$

A permutation matrix $P$ has a different arrange of the same rows as $I$. The example above permutes $row2$ and $row3$ of A.
> Easy to compute: there are $n!$ $P$ 's sized n by n (including $I$).

#### Breakdown of elimination

If there is a 'zero pivot', it is requiring _dividing by zero_ to get the multiplier, then elimination breaks down.

In such cases there could be an equation either with _infinitely many solutions_ like $0y = 0$  or _no solution_ like $0y = 1$.

A missing pivot means $A$ is `singular` and not `invertible`. 

### Inverse

> This section discusses square matrices only.
{: .prompt-warning}

Like $a^{-1}$ 'inverts' $a$ to give $a^{-1}a = 1$, we look for matrix inverses.

> Matrix $A$ is `invertible` if there exists a matrix that 'inverts' $A$.\
 Inverse is two sided: both $A^{-1}A = I$ and $AA^{-1} = I$ .
{: .prompt-info}

1. The inverse exists if and only if elimination produces n pivots(row exchanges are allowed.

> Equivalently $det A \neq 0$

2. A matrix CANNOT have two different inverses.

> Suppose $BA = I$ and $AC = I$ . Then \
$B(AC) = (BA)C$  gives  $BI = IC$ or $B = C$ \
This shows that the left-inverse and the right-inverse must be the same matrix.

3. Suppose there is a nonzero vector $\mathbf{x}$ such that $A\mathbf{x} = \mathbf{0}$. Then $A$ cannot have an inverse.

> No matrix can bring 0 back to $x$.

> Inverse of a product $AB$:\
$$(AB)^{-1} = B^{-1}A^{-1}$$\
Inverse of $AB$: $(AB)(B^{-1}A^{-1}) = AIA^{-1} = AA^{-1} = I$
{: .prompt-info}

> Same goes for $ABC$: $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ which is easy to prove.

### $A = LU$

> Assuming no row exchanges.

#### $LU$ decomposition

If $E$ subtracts $l_{21}row1$ from $row2$, then $E^{-1}$ adds $l_{21}row1$ from $row2$.

Now it's time to inverse the elimination: $E^{-1}EA = E^{-1}U$.

Again with the 3 by 3 example:

$$E^{-1} = E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} = 
	\begin{bmatrix} 1 & 0 &0\\l_{21} & 1 & 0\\0 & 0 & 1\end{bmatrix} 
	\begin{bmatrix} 1 & 0 &0\\0 & 1 & 0\\l_{22}& 0 & 1\end{bmatrix}
	\begin{bmatrix} 1 & 0 & 0\\0 & 1 & 0\\0 & l_{32} & 1\end{bmatrix}=
	\begin{bmatrix} 1 & 0 & 0\\l_{21} & 1 & 0\\l_{22} & l_{32} & 1\end{bmatrix}
	$$

The mutipliers sit exactly where they were!\
$E^{-1}$ is lower triangular, denoted $L$, so this produces $A = LU$, which is called the `LU decomposition`. 

With permutations: $PA = LU$ or $A = LUP$. $P$ could be anywhere.

> Any invertible matrix can be factored into $A = LUP = LDUP$.
{: .prompt-tip}

I find the conditions for the existence of $LU$ decomposition sort of complicated, so I'll just leave with this relatively weak statement.

> _**The key reason why this is easy**_: \
When pivot rows are subtracted from lower rows, they are not the original rows of $A$. Elimination changed them. However, they are exactly rows of $U$!

#### $A = LDU$

$L$ has the main diagonal all 1's, while $U$ is not. This is 'unsymmetric'. We could decompose $U$ into: $U = DU$. Now $D$ holds the pivots and $U$ has its diagonal all 1's.

This gives us $A = LDU$. For symmetric matrices $S = S^{T}$ it's $S = LDL^{T}$.

> We are using the same U, which is misleading, but they are different.

### Calculating $A^{-1}$: Guass-Joradan Elimination

#### Guass-Jordan

To calculate $\mathbf{x}$ in $A\mathbf{x} = \mathbf{b}$, $A^{-1}$ is not explicitly needed. Elimination goes _directly_ to $x$, and is also a way to compute $A^{-1}$.

Finding $A^{-1}$:

1. Start with 
$$\left[\begin{array}{c | c}A & I\end{array}\right]$$;
2. Eliminate downwards, _**and then upwards**_.
 Now A is diagonal.
3. Finally _divide by the pivots_ to get $I$ on the left. _**Then the right side is exactly $A^{-1}$!**_\
$$A^{-1}\left[\begin{array}{c | c}A & I\end{array}\right] = \left[\begin{array}{c | c}I & A^{-1}\end{array}\right]$$ Simple yet clever.

Finding $x$:

Augmenting $A$ with $b$: 
$\left[\begin{array}{c | c}A & \mathbf{b}\end{array}\right]$, 
carry on the same steps, and get 
$\left[\begin{array}{c | c}I & \mathbf{x}\end{array}\right]$.

> Here $A$ must be invertible. [Later section](#ax--b--the-complete-solution) gives a final solution for an arbitary $A$.

#### $*$ Recognizing an invertible matrix

A quick way:
> Diagonal dominance:\
For every $i$: $|a_{ii}| > \displaystyle\sum_{j \neq i}|a_{ij}|$\
Diagonally dominant matrices are invertible, but NOT vice versa.
{: .prompt-tip}
Each $a_{ii}$ on the diagonal is larger than the total sum of the rest of row $i$:

> Examples: $A$ is diagonally dominant and invertible; $B$ is not ... _but still invertible_. C is not ... and singular.\
$$A = \begin{bmatrix} 3 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 3\end{bmatrix}$$	 $$B = \begin{bmatrix} 2 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 1 & 3\end{bmatrix}$$		$$C = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 3\end{bmatrix}$$

**Reasoning:**

Take any nonzero vector $x$. Suppose its largest component is $|x_{i}|$, _**then $A\mathbf{x} = \mathbf{0}$ is impossible**_, because row $i$ of $A\mathbf{x} = \mathbf{0}$ would need\
$a_{i1}x_{1} + \dots + a_{ii}x_{i} + \dots + a_{in}x{n} = 0$.

Those can't add to zero whne $A$ is diagonally dominant! The size of $a_{ii}x_{i}$ is greater than all other terms combined:

$$\text{All } |x_{j}|\le|x_{i}|$$

$$\displaystyle\sum_{j \neq i}|a_{ij}||x{i}| < |a_{ii}||x_{i}| \text{ because } a_{ii} \text{ dominates }$$

This shows that $A\mathbf{x} = \mathbf{0}$ is only possible when $\mathbf{x} = \mathbf{0}$. _So A is invertible_.

## 3. Vector Spaces and Subspaces

### Vector Space, Subspaces

#### Vector spaces

> The `space` $\mathbf{R}^{n}$ consists of all column vectors $v$ with $n$ real components. 
{: .prompt-info}

Letter $\mathbf{R}$ is because components are real numbers. For complex components: $\mathbf{C}^n$.

$\mathbf{R}^{2}$ is represented by the usual $xy$ plane. Each vector gives the $x$ and $y$ coordinates of a point in the plane.

Similarly vectors in $\mathbf{R}^{3}$ correspond to points $(x, y, z)$ in the three-dimensiiontal spaces.

$\mathbf{R}^{0}$ contains only _**one**_(not zero!) vector: the $\mathbf{0}$. **This is also denoted as $\mathbf{Z}$**.

> Linear combinations of vectors in $\mathbf{R}^{n}$ produce vectors that stay in $\mathbf{R}^{n}$.
{: .prompt-info}

The meaning of vector space is not restricted to column vectors:

> $\mathbf{M}$ is the vector space of all real 2 by 2 _**matrices**_.\
$\mathbf{F}$ is the vector space of all real _**functions**_ $f(x)$.
{: .prompt-info}

In each case we can add: matrices to matrices, functions to functions. Multiply, too. The result is still in $\mathbf{M}$ or $\mathbf{F}$.

$\mathbf{F}$ is infinite-dimensional. A smaller function space is $\mathbf{P}$, or $$\mathbf{P}_{n}$$, containing all polynomials $$a_{0} + a_{1}x + \dots + a_{n}x^{n}$$ of degree n.

#### Subspaces

> A `subspace` of a vector space is a set of vectors (including $\mathbf{0}$) that satisfies two requirements:\
If $v$ and $w$ are vectors in the subspace and $c$ is any scalar, then
1. $v + w$ is in the subspace.
2. $cv$ is in the subspace.
{: .prompt-info}

In other words, the set of vectors is _'closed'_ under all linear combinations. 

> Example: Choose a plane in $\mathbf{R}^{3}$ through the origin $(0, 0, 0)$. That plane is a vector space in its own right. You can add, multiply vectors in the plane, the results are still in the plane.

Notice that in the above example the subspace is not $\mathbf{R}^{2}$! Even if it looks like.

> Example: subspaces inside $\mathbf{M}$ of all 2 by 2 matrices:\
$$(\mathbf{U})$$ All upper triangular matrices $$\begin{bmatrix} a & b \\ 0 & d\end{bmatrix}$$ of dimension: 3\
$$(\mathbf{D})$$ All diagonal matrices $$\begin{bmatrix} a & 0 \\ 0 & d\end{bmatrix}$$ of dimension: 2

> Every subspace has to contain the zero vector.
{: .prompt-tip}

### Independence, Dimension

#### Linear Independence

> Vectors $v_{1}, \dots, v_{n}$ are `linearly independent` if the only combination that gives $\mathbf{0}$ is the zero combination:\
$x_{1}v_{1} + x_{2}v_{2} + \dots + x_{n}v_{n} = \mathbf{0}$ only when all x's are 0.
{: .prompt-info}

Columns of $A$ are linearly independent when the only solution to $Ax = \mathbf{0}$ is $x = \mathbf{0}$. No other combinaton $Ax$ of the columns gives the zero vector.

Any set of n vectors in $\mathbf{R}^{m}$ must be dependent if $n > m$.

> A set of vectors `span` a space if their linear combinations fill the space.
{: .prompt-info}

Two vectors coming out from $(0, 0, 0)$ in 3-dimensional space could span a plane. Also they could only span a line -- if they're linearly dependent.

#### Basis

Two vectors can't span $\mathbf{R}^{3}$, even if they are independent. Four vectors can't be independent, even if they span $\mathbf{R}^{3}$.

> A `basis` for a vector space is a sequence of vectors that:
1. They are linearly _**independent**_.
2. They _**span**_ the space.
{: .prompt-info}

There is one and only one way to write $v$ as a combination of the basis vectors.

> Reason: Suppose $v = a_{1}v_{1} + \dots + a_{n}v_{n}$ and also $v = b_{1}v_{1} + \dots + b_{n}v_{n}$. By subtraction $(a_{1} - b_{1}) v_{1} + \dots + (a_{n} - b_{n}) v_{n} = \mathbf{0}$. From the independence of the $v$'s, each $a_{i} - b_{i} = 0$. Hence $a_{i} = b_{i}$. QED.

The columns of the n by n identity matrix give the `standard basis` for $\mathbf{R}^{n}$.

The basis is not unique. Columns of any invertible n by n matrix give a basis for $\mathbf{R}^{n}$.

> The only solution to $Ax = \mathbf{0}$ is $x = A^{-1}\mathbf{0} = \mathbf{0}$. So an invertible matrix has independent columns.

#### Dimension of a vector space

> The `dimension` of a space is the number of vectors in every basis.
{: .prompt-info}

There are infinitely many choices for the basis vectors, but the _number_ of basis vectors doesn't change.

If $v_{1}, \dots, v_{m}$ and  $w_{1}, \dots, w_{n}$ are both bases for the same vector space, then $m = n$.

> **Proof**:\
Suppose $n > m$. The $v$'s are a basis, so we can write every $w$ as a combination of the $v$'s:\
$$W = \begin{bmatrix} w_{1} & w_{2} & \dots & w_{n}\end{bmatrix} = 
\begin{bmatrix} v_{1} & \dots & v_{m} \end{bmatrix}
\begin{bmatrix} a_{11} & & a_{1n} \\ \vdots & & \vdots \\ a_{m1} & & a_{mn} \end{bmatrix} = VA$$\
The shape of $A$ is m by n, therefore $A$ can't be invertible, $Ax = \mathbf{0}$ _**has a nonzero solution**_. $Ax = \mathbf{0}$ gives $VAx = \mathbf{0}$ which is $Wx = \mathbf{0}$. Then the $w$'s cannot be a basis! The only way to avoid a contradiction is to have $m = n$.

The dimension of $\mathbf{Z}$ is zero. _**The empyt set**_ (containing no vectors) _**is a basis for**_ $\mathbf{Z}$. We can never allow $\mathbf{0}$ into a basis, because then independence is lost.

#### Bases for matrix spaces and function spaces

In differential equations, $d^{2}y/dx^{2} = y$ has a space of solutions. One basis is $y = e^{x}$ and $y = e^{-x}$. Counting the basis functions gives the dimension 2 for the space of all solutions. 

> The dimension is 2 because of second derivative.

**Matrix spaces**

The vector space $\mathbf{M}$ contains all 2 by 2 matrices. It's dimension is 4:

One basis is: $$A_{1}, A_{2}, A_{3}, A_{4} = \begin{bmatrix}1&0\\0&0\end{bmatrix}$$, $$\begin{bmatrix}0&1\\0&0\end{bmatrix}$$, $$\begin{bmatrix}0&0\\1&0\end{bmatrix}$$, $$\begin{bmatrix}0&0\\0&1\end{bmatrix}$$.

We are not looking at the columns, but the whole matrices:

$$c_{1}A_{1} + c_{2}A_{2} + c_{3}A_{3} + c_{4}A_{4} = \begin{bmatrix}c_{1}&c_{2}\\c_{3}&c_{4}\end{bmatrix} = A$$

$A$ is zero only if the $c$'s are all zero. So they are independent.

$A_{1}, A_{2}, A_{4}$ are a basis for a subspace of the upper triangular matrices. Similarly the diagonal matrices are also a subspace of $\mathbf{M}$.
- The dimension of the whole n by n matrix space is $n^{2}$.
- The dimension of the subspace of upper triangular matrices is $\frac{1}{2}n^{2} + \frac{1}{2}n$.
- The dimension of the subspace of diagonal mtrices is $n$.
- The dimension of the subspace of symmetric matrices is $\frac{1}{2}n^{2} + \frac{1}{2}n$.

**Function spaces**

In calculus we solve to find the functions $y(x)$:

- $y'' = 0$		is sovled by any linear function $y = cx + d$
- $y'' = -y$	is solved by any combination $y = c\sin{x} + d\cos{x}$
- $y'' = y$		is solved by any combination $y = ce^{x} + de^{-x}$

That solution space for $y'' = -y$ has two basis functions: $\sin{x}$ and $\cos{x}$. The space for $y'' = 0$ has $x$ and 1. It is the 'nullspace' of the second derivative! The dimension is 2 in each case.

The solution to $y'' = 2$ do not form a subspace - the right side $b = 2$ is not zero. A particular solution is $y(x) = x^{2}$. So the complete solution is $y(x) = x^{2} + cx + d$.

### the Four Subspaces

#### The column space of A: $\mathbf{C}(A)$

> The `column space` consists of all linear combinations of the columns.
{: .prompt-info}

The combinations are all possible vectors $Ax$. They fill the column space denoted $\mathbf{C}(A)$. _**To solve $Ax = b$ is to express $b$ as a combination of the columns**_. The right side $b$ has to be in the column space produced by $A$ on the left side, or there's no solution.

> The system $Ax = b$ is solvable if and only if b is in the column space of $A$. 
{: .prompt-tip}

The coefficients in that combination give us the solution $x$.

> For m by n matrix $A$, the column space $\mathbf{C}(A)$ is a subspace of $\mathbf{R}^{m}$ (not $\mathbf{R}^{n}$).
{: .prompt-info}

#### The nullspace of A: $\mathbf{N}(A)$ 

> The `nullspace` $\mathbf{N}(A)$ consists of all solutions to $Ax =0$. The vectors $x$ are in $\mathbf{R}^{n}$.
{: .prompt-info}

Check that the solution vectors form a subspace. Suppose $x$ and $y$ are in the nullspace. $Ax = 0$ and $Ay = 0$. Rules of matrix multiplication give $A(x + y) = 0 + 0$. The rules also give $A(cx) = c0$. Therefore their linear combinations are still in $\mathbf{N}(A)$, it is a subspace.

#### Row space and left nullspace

Similarly we have:

> The `row space` $\mathbf{C}(A^{T})$ is the subspace of $\mathbf{R}^{n}$ spanned by the rows. It's the column space of $A^{T}$.
{: .prompt-info}

> The `left nullspace` $\mathbf{N}(A^{T})$ consists of all solutions to $A^{T}y = 0$. The vectors $y$ are in $\mathbf{R}^{m}$.
{: .prompt-info}

This is the nullspace of $\mathbf{A}^{T}$. To solve $A^{T}y =0$ we solve $y{T}A = 0$. That's where the 'left' comes from.

#### Dimensions of the Four Subspaces

**Four Fundamental Subspaces:**

1. The row space is $\mathbf{C}(A^{T})$, a subspace of $\mathbf{R}^{n}$.
2. The column space is $\mathbf{C}(A)$, a subspace of $\mathbf{R}^{m}$.
3. The nullspace is $\mathbf{N}(A)$, a subspace of $\mathbf{R}^{n}$.
4. The left nullspace is $\mathbf{N}(A^{T})$, a subspace of $\mathbf{R}^{m}$.

> The row space and column space have the same dimenstion $r$.
{: .prompt-info}

$\mathbf{N}(A)$ and $\mathbf{N}(A^{T})$ have dimensions $n - r$ and $m- r$, to make the full $n$ and $m$.

### Solving $Ax = 0$, Rank

#### Free variables, special solutions

Perform elimination on $A$. If not invertible, we end up getting columns of no pivots or rows of all 0's, that's because they are linear combinations of the previous columns/rows.

$$C = \begin{bmatrix} 1 & 2 & 2 & 4 \\ 
					  3 & 8 & 6 & 16\\
					  2 & 4 & 4 & 8\end{bmatrix}
\hspace{0.5cm}\text{ becomes }\hspace{0.5cm}
U = \begin{bmatrix} \textbf{1} & 2 & 2 & 4 \\
					  0 & \textbf{2} & 0 & 4 \\
					  0 & 0 & 0 & 0\end{bmatrix}$$

This is `row echelon form`.

Here row1 and row2 are pivot rows. col1 and col2 are `pivot columns`, and col3 and col4 containing no pivots are `free columns`.

`Free variables` correspond to free columns, they are free because they make no difference to solvability. Normally we set one of them to 1 and others to 0, to find each `special solution`.

$$ s_{1} = \begin{bmatrix} -2 \\ 0 \\ 1 \\ 0 \end{bmatrix}
\hspace{0.5cm}\text{ and }\hspace{0.5cm}
s_{2} = \begin{bmatrix} 0 \\ -2 \\ 0 \\ 1 \end{bmatrix}$$

**When we have one special solution, we have a whole subspace of them:**

$$ x = c_{1}s_{1} + c_{2}s_{2} = c_{1}\begin{bmatrix} -2 \\ 0 \\ 1 \\0\end{bmatrix} + c_{2}\begin{bmatrix} 0 \\ -2 \\ 0 \\ 1 \end{bmatrix}$$

#### Reduced row echelon form $R$

Look back to where the elimination ended. We perform 2 more steps:

1. **Produce 0's above the pivots** by eliminating backwards.
2. **Produce 1's in the pivots** by dividing each pivot row by its pivot.

Those steps don't change the zero vector on the right hand side. The nullspace stays the same: $\mathbf{N}(A) = \mathbf{N}(U) = \mathbf{N}(R)$, and it's easiest to see when we reach this `reduced row echelon form` $R = rref(A)$.

$$U = \begin{bmatrix} 1 & 2 & 2 & 4 \\
					  0 & 2 & 0 & 4 \\
					  0 & 0 & 0 & 0\end{bmatrix}
\hspace{0.5cm}\text{ becomes }\hspace{0.5cm}
R = \begin{bmatrix} \textbf{1} & \textbf{0} & 2 & 0 \\
					  \textbf{0} & \textbf{1} & 0 & 2 \\
					  0 & 0 & 0 & 0\end{bmatrix}$$

_**The pivot columns of $R$ contain $I$.**_

Now the pattern is clear, give a 4 by 5 example:

$$A = \begin{bmatrix} \mid & \mid & \mid & \mid & \mid \\
					  p & p & f & p & f\\
					  \mid & \mid & \mid & \mid & \mid \end{bmatrix}$$

- to be revealed by $R$:
- 3 pivot columns $p$
- 2 free columns $f$

$$R = \begin{bmatrix} \textbf{1} & 0 & a & 0 & c\\
					  0 & \textbf{1} & b & 0 & d\\
					  0 & 0 & 0 & \textbf{1} & e\\
					  0 & & \dots & & 0\end{bmatrix}$$

- $I$ in pivot columns
- $F$ in free columns
- 3 pivots: rank $r = 3$

$$s_{1} = \begin{bmatrix} -a \\ -b \\ \textbf{1} \\ 0 \\ \textbf{0} \end{bmatrix}
\hspace{0.5cm}
s_{2} = \begin{bmatrix}  -c \\ -d \\ \textbf{0} \\ -e \\ \textbf{1} \end{bmatrix}$$

- special $Rs_{1} = \mathbf{0}$ and $Rs_{2} = \mathbf{0}$
- take $-a$ to $-e$ from $R$
- $Rs = \mathbf{0}$ means $As = \mathbf{0}$

Rearrange columns of $R$ and rows of $S$ and they become:

> $$RS = \begin{bmatrix} I & \mid & F \\
					 \cdots & 0 & \cdots \end{bmatrix}
	\begin{bmatrix} -F \\ I \end{bmatrix} = \mathbf{0}$$
{: .prompt-tip}

Notice the _**solution $S$ has changed**_ due to column exchanges. Don't forget to switch the rows back.

#### Rank

> #pivot $=$ `rank` $r$
{: .prompt-info}

The rank $r$ is the 'dimension' of the column space. It is also the 'dimension' of the row space. 

> Notice _rank_ is for matrices while _dimension_ is for spaces.

> **Full column rank**	$r = n$\
**Full row rank**	$r = m$
{: .prompt-info}

The columns of $A$ are independent exactly when $r = n$. There are $n$ pivots and no free variables. Only $x = \mathbf{0}$ is in the nullspace.

> In form of $R = rref(A)$, the pivot rows of $R$ are a basis for its row space, the pivot columns of $A$ (NOT $R$ !) are a basis for its column space.
{: .prompt-tip}

During row operations, we are really taking linear combinations of the rows, whereas the column space is changed! Although the dimension does stay the same.
Correspondingly, $A$ has the same nullspace as $R$, not the left nullspace.

> For a cleaner choice you may want to look at $A = LDR$. The column space of $A$ is the same as $L$, while the latter is nicer shaped. 

### $Ax = b$ : The Complete Solution

#### Augmented matrix, condition

Augment $b$ to $A$, and carry on elimination:

$$\left[\begin{array}{c | c} A & b \end{array}\right] =
\left[\begin{array}{c c c c | c} 1 & 3 & 0 & 2 & b_{1} \\
								 0 & 0 & 1 & 4 & b_{2} \\
								 1 & 3 & 1 & 6 & b_{3} \end{array}\right]
{\large\to}
\left[\begin{array}{c c c c | c} 1 & 3 & 0 & 2 & b_{1} \\
								 0 & 0 & 1 & 4 & b_{2} \\
								 0 & 0 & 0 & 0 & b_{3} - b_{2} - b_{1} \end{array}\right] =
\left[\begin{array}{c | c} R & d \end{array}\right]$$

Notice the system is solvable only if $b_{3} - b_{2} - b_{1} = 0$.

> This also indicates the vectors in the left nullspace: $- row_{1} - row_{2} + row_{3} = 0$.
{: .prompt-tip}

#### The complete solution

Choose the free variables to be 0. Then we find one `particular solution`.

- The particular solution solves $Ax_{p} = b$
- The $n - r$ special solutions solve $Ax_{n} = \mathbf{0}$

> Complete solution: one $x_{p}$ many $x_{n}$\
$$x = x_{p} + c_{1}x_{1} + \dots + c_{n}x_{n}$$
{: .prompt-tip}

#### Full row rank

If a matrix $A$ has full row rank:

1. All rows have pivots, and $R$ has no zero rows.
2. $Ax = b$ _**has one or $\infty$ solutions for every**_ $b$.
3. The column space is the whole $\mathbf{R}^{m}$.
4. There are $n - r = n - m$ special solutions in the nullspace.

$$ \textbf{Full row rank} \hspace{1cm} \begin{matrix} x & + & y & + & z & = & 3 \\ x & + & 2y & - & z & = & 4 \end{matrix} \hspace{1cm} r = m = 2$$

![P153](153.png)

These are two planes in $\mathbf{R}^{3}$ that intersects in a line.

The particular solution will be one point on the line. Adding the nullspace vectors $x_{n}$ will move the point along the line.

#### Full column rank

For any full column rank matrix:

1. All columns are pivot columns.
2. No free variables or special solutions.
3. $\mathbf{N}(A)$ contains the only $\mathbf{0}$.
4. $Ax = b$ has _**0 or 1 solution**_.

Look at the column picture: For vector $b$ inside the column space, there is only one solution because the columns are independent. Outside of the column space, there's no linear combination that will give the right $b$.

#### Conclusion

|$r = m, \space r = n$|Square and invertible|$Ax = b$ has 1 solution|$T$ is both _one-to-one_ and _onto_|
|$r = m, \space r < n$|Full row rank|$Ax = b$ has $\infty$ solutions|$T$ is _onto_|
|$r < m, \space r = n$|Full column rank|$Ax = b$ has 0 or 1 solution|$T$ is _one-to-one_|
|$r < m, \space r < n$|Not full rank|$Ax = b$ has 0 or $\infty$ solutions||

> The last statement involves terms from [linear transformations](#6-linear-transformations).

#### Invertible matrix theorem

> Invertible Matrix Theorem:\
Let $A$ be an $n$ by $n$ matrix, and $T: R^n \longrightarrow R^n$ be the matrix transformation $T(x) = Ax$. The following statements are _equivalent_:
- $A$ is invertible.
- $A$ has $n$ pivots.
- $N(A)$ = $\{0\}$.
- Columns of $A$ are linearly independent.
- Columns of $A$ span $R^n$.
- $Ax = b$ has a unique solution for each $b$ in $R^n$.
- $T$ is invertible.
- $T$ is _one-to-one_.
- $T$ is _onto_.
{: .prompt-info}

> The last statements involve terms from [linear transformations](#6-linear-transformations).

### Orthognality

#### Orthogonality

$$\textbf{Orthogonal vectors}\hspace{2cm} v^{T}w = 0 \hspace{1cm} \text{and} \hspace{1cm} ||v||^{2} + ||w||^{2} = ||v + w||^{2}$$

> Two subspaces $V$ and $W$ of a vector space are `orthogonal` when:\
$v^{T}w = 0$ for all $v$ in $V$ and all $w$ in $W$.
{: .prompt-info}

> Example: The $xy$ plane and $yz$ plane in $xyz$ space are NOT orthogonal because they intersect in a line, even though they are perpendicular. The $xy$ plane and $z$ axis are orthogonal.

> The only intersection of two orthogonal subspaces is $\mathbf{0}$.
{: .prompt-tip}

#### Orthogonal complements

> The `orthogonal complement` of a subspace $\mathbf{V}$ contains every vector that's orthogonal to $\mathbf{V}$, denoted $\mathbf{V}^{\perp}$.
{: .prompt-info}

#### Orthognality of the Four Subspaces

- The row space is orthogonal to the null space.

In $Ax = 0$ the dot product of every row of $A$ with every $x$ in the nullspace gives 0.

- The column space is orthogonal to the left nullspace.

In $Ax = b$ when $b$ is outside the column space, the left nullspace comes into its own -- It contains the error $e = b - Ax$ in the least squares solution.

![P197](197.png)

> - $\mathbf{N}(A)$ is the orthogonal complement of $\mathbf{C}(A^{T})$ (in $\mathbf{R}^{n}$).
- $\mathbf{N}(A^{T})$ is the orthogonal complement of $\mathbf{C}(A)$ (in $\mathbf{R}^{m}$).
{: .prompt-info}

> Every $x$ can be split into a row space component $x_{r}$ and a nullspace component $x_{n}$.
{: .prompt-tip}

> Every $b$ in the $C(A)$ comes from one and only one $x_{r}$ in $C(A^T)$.
{: .prompt-info}

Using terms that are yet to be introduced, a matrix is a linear transformation that takes vectors in $C(A^T)$ to $C(A)$. It's a mapping from $R^n$ to $R^m$.

> Proof:\
If $$Ax_{r} = Ax'_{r}$$, the difference $$x_{r} - x'_{r}$$ is in the nullspace. It is also in the row space, where $$x_{r}$$ and $$x'_{r}$$ came from. So the difference must be the zero vector: $$x_{r} = x'_{r}$$.

- There is an $r$ by $r$ invertible matrix hiding inside $A$, if we throw away the two nullspaces. From the row space to the column space, $A$ is "invertible".

![P199](199.png)

### Projections

#### Projection onto a line

Consider vectors $b$ and $a$ that are not on the same line. Projecting $b$ onto a line through $a$ produces the results:
1. Projection $p$ will be some multiple of $a$: $p = \hat{x}a$. 
2. `error` $e = b - p$ will be orthogonal to $a$: $a^{T}(b - p) = 0$.

$$\displaystyle \begin{align}\begin{split}
a^{T}(b - p) & = 0 \hspace{0.5cm} \text{and} \hspace{0.5cm} p = \hat{x}a\\
a^{T}b & = a^{T}\hat{x}a\\
\hat{x} & = {a^{T}b \over a^{T}a}
\end{split}\end{align}$$

> Observe the relation: _Dot product_ $a^T b = \hat{x}\vert\vert a\vert\vert$.

> The `projection` of $b$ onto a line through $a$ is:\
$$\displaystyle p = \hat{x}a = {a^{T}b \over a^{T}a} a$$
{: .prompt-info}

Check the two special cases:

1. If $b = a$ then $\hat{x} = 1$. $Pa = a$.
2. If $b \perp a$ then $a^{T}b = 0$. $p = 0$.

Now comes to `projection matrix`. In the formula, what matrix is multiplying $b$ ? You can see the matrix better if $\hat{x}$ is on the right side of $a$:

$$\displaystyle p = a\hat{x} = a{a^{T}b \over a^{T}a} = Pb \hspace{1cm} P =  {aa^{T} \over a^{T}a}$$

$P$ is a column times a row then divided by a number. It's m by m of rank 1. We are projecting onto a 1-dimensional subspace, as predicted. It's the column space of $P$ and the line.

#### Projection onto a subspace

Same idea: With a basis: $a_{1}, \dots, a_{n}$ in $\mathbf{R}^{m}$, **find the combination** $$p = \hat{x}_{1}a_{1} + \dots + \hat{x}_{n}a_{n}$$ **closest to** $b$. The error vector $$b - A\hat{x}$$ is orthogonal to the subspace.

$$\displaystyle \begin{align}
\begin{matrix}
a_{1}^{T}(b - A\hat{x}) = 0 \\
\vdots \\
a_{n}^{T}(b - A\hat{x}) = 0 
\end{matrix}
\hspace{0.5cm}\text{or}\hspace{0.5cm}
\begin{bmatrix}
	- & a_{1}^{T} & - \\
	   & \vdots & \\
	- & a_{n}^{T} & - \end{bmatrix}
\begin{bmatrix} b - A\hat{x} \end{bmatrix}
= \begin{bmatrix} 0 
\end{bmatrix}
\end{align}
$$

This is

$$\displaystyle
\begin{align}
A^{T}(b - A\hat{x}) & =  \mathbf{0} \\
A^{T}A\hat{x}		& =  A^{T}b
\end{align}
$$

The symmetric matrix $A^{T}A$ is $n$ by $n$. It's invertible if the $a$'s are independent. 

$$\begin{align}\begin{split}
\text{The solution is}\\ 
\hat{x} \space (n \times 1) & \hspace{1cm} \hat{x} = (A^{T}A)^{-1}A^{T}b \\
\text{The projection is}\\
 p \space (m \times 1) & \hspace{1cm} p = A\hat{x} = A(A^{T}A)^{-1}A^{T}b\\
 P \space (m \times m)  & \hspace{1cm} P = A(A^{T}A)^{-1}A^{T}
\end{split}\end{align}$$

> You may attempt to cancel all the $A$'s and get $P = I$, but notice that $A$ could be rectangular!
{: .prompt-warning}

> $A^{T}A$ is invertible if and only if $A$ has independent columns.
{: .prompt-tip}

$A^{T}A$ is square $(n \times n)$. If columns of $A$ are independent, then $$N(A) = \{0\}$$. Then $A^{T}A$, with the same nullspace, is invertible.

> In [Positive Definiteness](#positive-definiteness), this $A^TA$ is not just invertible, but _positive definite_. When $A$ does not have full column rank, $A^TA$ is _positive semi-definite_.

> **Important fact**: $P^{2} = P$ : Projecting a second time doesn't change anything.\
$$P^{2} = A(A^{T}A)^{-1}(A^{T}A)(A^{T}A)^{-1}A^{T} = A(A^{T}A)^{-1}A^{T} = P$$
{: .prompt-tip}

### Least Squares: $Ax \neq b$

#### Why $\hat{x}$ ?

It often happens that $Ax = b$ has no solution because of _more equations than unknowns_. $b$ is outside $\mathbf{C}(A)$. When $\vert\vert e \vert\vert$ is as small as possible, $$\hat{x}$$ is a `least squares` solution. 

Geometrically, we are looking for a point in $\mathbf{C}(A)$ closest to $p$. That point is the projection $p$. 

Algebracally, every $b$ can be split into two parts: $p$ in $\mathbf{C}(A)$ and $e$ in $\mathbf{N}(A^{T})$. 

$$b = p + e$$

We solve $A\hat{x} = p$ by removing $e$ and solving $A^{T}A\hat{x} = A^{T}b$.

![P222](222.png)

#### Fitting a straight line

$$\displaystyle
\text{Ax = b}\hspace{0.5cm}\text{is}\hspace{0.5cm}
\begin{matrix}
C + Dt_{1} = b_{1}\\
C + Dt_{2} = b_{2}\\
\vdots \\
C + Dt_{m} = b_{m}\end{matrix}
\hspace{0.5cm}\text{with}\hspace{0.5cm}
A = \begin{bmatrix}
1 & t_{1} \\ 1 & t_{2} \\ \vdots & \vdots \\ 1 & t_{m} 
\end{bmatrix}$$

The closest line $p = \hat{C} + \hat{D}t$ has heights $p_{1}, \dots , p_{m}$ with errors $e_{1}, \dots , e_{m}$.

![P221](221.png)

Solve $A^{T}A\hat{x} = A^{T}b$  for $\hat{x} = (\hat{C}, \hat{D})$. The errors are $e_{i} = b_{i} - \hat{C} - \hat{D}t_{i}$.

$$\displaystyle\begin{align}
A^{T}A = \begin{bmatrix} 1 & \dots & 1 \\ t_{1} & \dots & t_{m} \end{bmatrix}
\begin{bmatrix} 1 & t_{1} \\ \vdots & \vdots \\ 1 & t_{m} \end{bmatrix}
& = \begin{bmatrix} m & \sum t_{i} \\ \sum t_{i} & \sum {t_{i}}^{2} \end{bmatrix} \\
A^{T}b  = \begin{bmatrix} 1 & \dots & 1 \\ t_{1} & \dots & t_{m} \end{bmatrix}
\begin{bmatrix} b_{1} \\ \vdots \\ b_{m} \end{bmatrix}
& = \begin{bmatrix} \sum b_{i} \\ \sum t_{i}b_{i}\end{bmatrix}\\
A^{T}A\hat{x} = A^{T}b \hspace{1cm} 
\begin{bmatrix} m & \sum t_{i} \\ \sum t_{i} & \sum {t_{i}}^{2} \end{bmatrix}
& \begin{bmatrix} \hat{C} \\ \hat{D} \end{bmatrix}
= \begin{bmatrix} \sum b_{i} \\ \sum t_{i}b_{i}\end{bmatrix}
\end{align}$$

> Interesting facts:
1. $A$ has orthogonal columns when the measurement times $t_{i}$ add to 0.
2. When $A$ has orthogonal columns, $A^{T}A$ is diagonal.
{: .prompt-tip}

Example: 

$$\displaystyle\begin{align}
Ax = \begin{bmatrix} 1 & -2\\ 1 & 0 \\ 1 & 2 \end{bmatrix}
\begin{bmatrix} C \\ D \end{bmatrix}
& = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix} \\
A^{T}A\hat{x} = A^{T}b \hspace{0.5cm}
\begin{bmatrix} 3 & \mathbf{0} \\ \mathbf{0} & 8 \end{bmatrix}
\begin{bmatrix} \hat{C} \\ \hat{D} \end{bmatrix}
& = \begin{bmatrix} 7 \\ 6 \end{bmatrix}
\end{align}$$

Orthogonal columns are so helpful that it's worth _shifting the times by subtracting the average time_ $\bar{t} = \sum t_{m} / m$. This is valid because we have a column of all 1's. We are not changing the column space, but really the basis.

$$A = \begin{bmatrix} 1 & 1 \\ 1 & 3 \\ 1 & 5 \end{bmatrix}
\to A' = \begin{bmatrix} 1 & -2 \\ 1 & 0 \\ 1 & 2 \end{bmatrix}$$

Notice that the best line now is $p = \hat{C} + \hat{D}(t - \bar{t}) = (\hat{C} - \hat{D}\bar{t}) + \hat{D}t$.  $\hat{C}$ _**is changed!**_

Later, we'll orthogonalize $A = QR$ to reduce the process.

From the start, we have assumed independent columns in $A$, then $A^{T}A$ is invertible. Which $\hat{x}$ is best for _dependent columns_? Answer: $A\hat{x}$ will be a horizontal line in height of the average of the measurements.

#### Fitting a parabola

$$
\begin{matrix}
C + Dt_{1} + E{t_{1}}^{2} = b_{1} \\
\vdots \\
C + Dt_{m} + E{t_{m}}^{2} = b_{m}
\end{matrix}
\hspace{1cm}
A\hat{x} = \begin{bmatrix}
1 & t_{1} & {t_{1}}^{2} \\
\vdots & \vdots & \vdots \\
1 & t_{m} & {t_{m}}^{2} \end{bmatrix}
\begin{bmatrix} \hat{C} \\ \hat{D} \\ \hat{E} \end{bmatrix}
$$

### Orthogonal Bases

#### Orthonormals

> `Orthonormal` vectors $q_{1}, \dots, q_{n}$ have:\
$$\displaystyle q_{i}^{T}q_{j} = \begin{cases}
0 & \text{when} i \neq j & \text{(orthogonal)} \\
1 & \text{when} i = j    & \text{(unit)} \end{cases}$$\
Letter $Q$ is for matrix with orthonormal vectors.\
In square cases we call $Q$ an `orthogonal matrix`.
{: .prompt-info}

> Therefore, a matrix $Q$ satisfies $Q^{T}Q = I$.\
 When $Q$ is square, this means that $Q^{T} = Q^{-1}$ : transpose = inverse.
{: .prompt-tip}

Examples:

- (**Rotation**) $Q$ rotates every vector in the plane by angle $\theta$ :

$$Q = \left[\begin{array}{rr}
\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{array}\right]
\hspace{0.5cm}\text{and}\hspace{0.5cm}
Q^{T} = Q^{-1} = \left[\begin{array}{rr}
\cos\theta & \sin\theta \\ -\sin\theta & \cos\theta
\end{array}\right]$$

- (**Permutation**) :

$$P = \dots$$

- (**Reflection**) If $u$ is any unit vector, set $Q = I - 2uu^{T}$. 

$$Q^{T} = I - 2uu^{T} = Q
\hspace{0.5cm}\text{and}\hspace{0.5cm}
Q^{T}Q = I - 4uu^{T} + 4uu^{T}u^{T} = I$$

Reflection matrices $I - 2uu^{T}$ are symmetric and orthogonal. Squaring them you get $Q^{2} = Q^{T}Q = I$. Reflecting twice brings back the original.

![P235](235.png)

> Example: choose the direction $u = (-1/\sqrt{2}, 1/\sqrt{2})$. \
$$\text{Reflection}\hspace{0.2cm}
Q = I - 2\left[\begin{array}{rr} .5 & -.5 \\ -.5 & .5 \end{array}\right]
= \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
\hspace{0.3cm}\text{and}\hspace{0.3cm}
\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
= \begin{bmatrix} y \\ x \end{bmatrix}$$

Rotations, permutations, reflections all _**preserve the length**_ of input vector.

> 
- If $Q^{T}Q = I$, $\vert\vert Qx \vert\vert = \vert\vert x \vert\vert$ for every x:\
$$(Qx)^{T}(Qx) = x^{T}Q^{T}Qx = x^{T}Ix = x^{T}x$$
- $Q$ also preserves dot products:\
$$(Qx)^{T}(Qy) = x^{T}Q^{T}Qy = x^{T}y$$
{: .prompt-tip}

> Make sure to check this section again after exposure to concepts of linear transformation and change of basis.

#### Projections using orthonormal bases

For projections onto subspaces, all formulas involve $A^{T}A$. The entries of $A^{T}A$ are $a_{i}^{T}a_{j}$ of the basis $a_{1}, \dots , a_{n}$.

It'd be nice if the basis vectors are orthonormal. Then $A^{T}A$ simplifies to $Q^{T}Q  = I$. Look at the improvements:

$$\begin{align}
A^{T}A\hat{x} &= A^{T}b &
p &= A(A^{T}A)^{-1}A^{T}b&
P &= A(A^{T}A)^{-1}A^{T}
\\ &{\large\downarrow}& &{\large\downarrow}& &{\large\downarrow}& \\
\hat{x} &= Q^{T}b&
p &= QQ^{T}b&
P &= QQ^{T}
\end{align}$$

$$\text{Projection}\hspace{0.5cm}
p = Q\hat{x} = QQ^{T}b = 
\begin{bmatrix} \vert & & \vert \\ q_{1} & \dots & q_{n} \\ \vert & & \vert \end{bmatrix}
\begin{bmatrix} q_{1}^{T}b \\ \vdots \\ q_{n}^{T}b \end{bmatrix}
= q_{1}(q_{1}^{T}b) + \dots + q_{n}(q_{n}^{T}b)
$$

An important special case: 

When $Q$ is square, the subspace is the whole space. Then $Q^{T} = Q^{-1}$ and $\hat{x} = Q^{T}b$ is the same as $x = Q^{-1}b$. The solution is exact. Projection of $b$ onto the whole space is $b$ itself. $p = b$ and $P = QQ^{T} = I$.

This projection onto the whole space may seem not worth mentioning. But when $p = b$, the formula assembles $b$ out of its 1-dimensional projections. Every $b = QQ^{T}b$ _is the sum of its components along the $q$'s_.

$$\displaystyle b = q_{1}(q_{1}^{T}b) + \dots + q_{n}(q_{n}^{T}b)
$$

$QQ^{T} = I$ is the foundation of all the great _transforms_ like Fourier series.

### Gram-Schmidt, $A = QR$

#### The Gram-Schmidt process

This is about creating orthonormal vectors.

Start with independent $a, b, c, \dots$. We intend to reach orthogonal $A, B , C, \dots$. Then divide each by its length to produce orthonormal vectors.

First, choose $A = a$. Then, subtract from $b$ by its projection along $A$. This leaves the orthogonal part $B$ that we want.

$$\text{First Gram-Schmidt step}\hspace{1cm}
B = b - {A^{T}b \over A^{T}A}A$$

In previous sections $e = B$ is what we wanted to exclude. Now it's different. 

> If you end up with $B = 0$, it's because of dependent original vectors.

$$\text{Next Gram-Schmidt step}\hspace{1cm}
C = c - {A^{T}c \over A^{T}A}A - {B^{T}c \over B^{T}B}B$$

The next steps are very clear after we know what we do for the first step. This is the only idea of `Gram-Schmidt` process:

> Gram-Schmidt:
1. Orthogonalize: Subtract from every new vector its projections in the directions set by the existing orthonormals. 
2. Normalize: Divide by length.
{: .prompt-info}

![P238](238.png)

Normalize:

$$q_{1} = A / ||A|| , q_{2} = B / ||B||, \dots , q_{n} = X / ||X||$$

#### $A = QR$

- $a, A, q_{1}$ are all along a line.
- $a, b, A, B, q_{1}, q_{2}$ are all in the same plane.
- $a, b, c, A, B, C, q_{1}, q_{2}, q_{3}$ are all in one 3-dimensional subspace.

At every step $a_{1}, \dots, a_{k}$ are combinations of $q_{1}, \dots, q_{k}$. Later $q$'s are not involved. This non-involvement of later vectors is the key point of $A = QR$:

$${\large A = QR}\hspace{1cm}
\begin{bmatrix} 
\vert & \vert & \vert \\
a_{1} & a_{2} & a_{3} \\
\vert & \vert & \vert
\end{bmatrix}
= \begin{bmatrix} 
\vert & \vert & \vert \\
q_{1} & q_{2} & q_{3} \\
\vert & \vert & \vert
\end{bmatrix}
\begin{bmatrix}
q_{1}^{T}a_{1} & q_{1}^{T}a_{2} & q_{1}^{T}a_{3} \\
		   		& q_{2}^{T}a_{2} & q_{2}^{T}a_{3} \\
		   		&				& q_{3}^{T}a_{3}
\end{bmatrix}
$$

> $R = Q^{T}A$ is **_upper triangular_** because later $q$'s are orthogonal to earlier $a$'s.
{: .prompt-tip}

> **Any** matrix $A$ can be factored into $A = QR$.
{: .prompt-tip}

This is literally _any_ : even rectangular matrices with no full rank.

This is also worth noting:

> The main diagonal of $R: q_{1}^{T}a_{1}, \dots, q_{n}^{T}a_{n}$ is the _lengths_ of $a$'s.
{: .prompt-tip}

We must not forget why this is useful to least squares:

$$A^{T}A = (QR)^{T}(QR) = R^{T}Q^{T}QR = R^{T}R$$

This simplifies the work:

$$\text{Least squares}\hspace{0.5cm}\\ \begin{align}
A^{T}A\hat{x} &= A^{T}b &
p &= A(A^{T}A)^{-1}A^{T}b&
P &= A(A^{T}A)^{-1}A^{T}
\\ &{\large\downarrow}& &{\large\downarrow}& &{\large\downarrow}& \\
R\hat{x} &= Q^{T}b&
p &= QRR^{-1}Q^{T}b = QQ^{T}b&
P &= QQ^{T}
\end{align}$$

# Second Half: Eigenvalues and Eigenvectors

## 4. Determinants

### The Properties of Determinants

#### 3 basic properties

We want the determinant of a matrix to contain a certain amount of information about that matrix. For example, the determinant is 0 when the matrix has no inverse. When $A$ is invertible, $det A^{-1} = 1/(det A)$.

Determinant is written in two ways: $\det A$ and $\vert A \vert$.

`Determinant` is determined by 3 **basic properties**:

> 
1. $\det I = 1$.
2. Sign reversal: The determinant changes sign upon each row exchange.
3. Linearity: The determinant is a linear function of each row separately:
$$\begin{align}\text{multiply}\hspace{1cm}
&\begin{vmatrix} ta & tb \\ c & d \end{vmatrix} = t\begin{vmatrix} a & b \\ c & d \end{vmatrix} \\
\text{add}\hspace{1cm}
&\begin{vmatrix} a + a' & b + b' \\ c & d \end{vmatrix}
= \begin{vmatrix} a & b \\ c & d \end{vmatrix}+
\begin{vmatrix} a' & b' \\ c & d \end{vmatrix}
\end{align}$$
{: .prompt-info}

With these 3 properties, we will find properties 4 - 10.

#### More properties

> 4.\
If two rows are equal, $\det A = 0$.
{: .prompt-info}

Exchange the two equal rows and get: $-\det A = \det A$. So the determinant must be 0.

> 5.\
Subtracting a multiple of one row from another leaves $\det A$ unchanged.\
$$\begin{vmatrix} a & b \\ c - la & d - lb \end{vmatrix}
= \begin{vmatrix} a & b \\ c & d \end{vmatrix}
$$
{: .prompt-info}

This is easy to see by the steps of subtraction.

> **Conclusion**: $\det A$ is not changed by usual elimination steps (no permutation) from $A$ to $U$.
{: .prompt-tip}

> 6.\
A matrix with a row of zeros has $\det A = 0$.
{: .prompt-info}

Add some other row to the zero row, and $\det A$ is not changed with now two equal rows.

> 7.\
If $A$ is triangular then $\det A = a_{11}a_{22}\dots a_{nn}=$ product of diagonal entries.
{: .prompt-info}

Remove the off-diagonal entries by elimination, and the pivot values stay the same.

> 8.\
Invertible $A \leftrightarrow \det A \neq 0$.\
Singular $A \leftrightarrow \det A  = 0$.
{: .prompt-info}

Elimination goes from $A$ to $U$. If $A$ is singular then $U$ has a zero row: $\det A = \det U = 0$. Otherwise $A$ is invertible with pivots along the diagonal. The product along the diagonal of this triangular matrix gives a nonzero determinant.

$$\text{determinant}(2 \times 2)\hspace{1cm}
\begin{vmatrix} a & b \\ c & d \end{vmatrix} =
\begin{vmatrix} a & b \\ 0 & d - (c/a)b \end{vmatrix}
= ad - bc$$

This is our first formula for the determinant.

> 9.\
Product rule: $$\vert AB \vert = \vert A \vert \vert B \vert$$
{: .prompt-info}

This rule is the most intricate so far, even with a 2 by 2 case there's some algebra. For the $n$ by $n$ case, here is a snappy proof:

> 
When $\vert B \vert = 0$, $AB$ is singular and $\vert AB \vert$ is 0.\
When $\vert B \vert \neq 0$, consider $d = \vert AB \vert / \vert B \vert$. This ratio has properties 1, 2, 3, then $d$ has to be $\det A$ and we have $\vert AB \vert / \vert B \vert = \vert A \vert$.
- Determinant of $I$: If $A = I$ then $d = \vert B \vert / \vert B \vert = 1$.
- Sign reversal: When two rows of $A$ are exchanged, so are the same two rows of $AB$. Therefore $\vert AB \vert$ changes sing and so does $d = \vert AB \vert / \vert B \vert$.
- Linearity: When row 1 of $A$ is multiplied by $t$, so is row 1 of $AB$. This multiplies $\vert AB \vert$ by $t$. So $d = \vert AB \vert / \vert B \vert$ is multiplied by $t$.\
Add row 1 of $A$ to row 1 of $A'$. Then row 1 of $AB$ adds to row 1 of $A'B$. By rule 3, determinants add. Dividing by $\vert B \vert$, the ratios add - as desired.

When $B$ is $A^{-1}$, the rule says that $\det A^{-1} = 1/\det A$.

> $AA^{-1} = I \rightarrow (\det A)(\det A^{-1}) = \det I = 1 \rightarrow \det A^{-1} = 1/\det A$
{: .prompt-tip}

> 10.\
Transpose rule: $\vert A^{T} \vert = \vert A \vert$
{: .prompt-info}

For singular matrices the rule is clear that $0 = 0$. For invertible matrices: First it's easy to show that transpose of triangular matrices doesn't change the determinants. Then transpose of permutations also, because $P^{T}P = I$. And $P$ $L$, $U$ gives any matrices $\det A = \det A^{T}$.

> Rule 10 shows that every rule for the rows also applies to columns.
{: .prompt-tip}

> Notice that there is NO such rule like $\vert A \vert + \vert B \vert = \vert A + B \vert$. Counterexamples are easy to come up with, like subtracting to get a zero row.
{: .prompt-warning}

### Cofactors and Permutations

So far we've learned one systematic way to compute determinants: by elimination and multiplying the pivots. Here are two more of them:

#### The "big formula" for determinants

Look at this 2 by 2 example:

$$\begin{align}
\begin{vmatrix} a & b \\ c & d \end{vmatrix}
& = \begin{vmatrix} a & 0 \\ c & d \end{vmatrix} +
\begin{vmatrix} 0 & b \\ c & d \end{vmatrix} 
&\text{break up row 1}
\\
& = \begin{vmatrix} a & 0 \\ 0 & d \end{vmatrix} +
\begin{vmatrix} a & 0 \\ c & 0 \end{vmatrix} + 
\begin{vmatrix} 0 & b \\ c & 0 \end{vmatrix} + 
\begin{vmatrix} 0 & b \\ 0 & d \end{vmatrix}
&\hspace{0.5cm}\text{break up row 2}
\\
& = ad \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix} + bc \begin{vmatrix} 0 & 1 \\ 1 & 0 \end{vmatrix} 
\\
& = ad - bc
\end{align}$$

The pattern seems to emerge: $n^{2}$ terms for $n$ by $n$ matrix. Some is 0. Some has plus sign and some has minus. Check the 3 by 3 example to see more clearly:

$$
\begin{align}
\begin{vmatrix} 
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{vmatrix} = 
& +a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}
\\
& -a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}
\end{align}$$

Each term turns out to be something like $a_{11}a_{22}a_{33} P$. From row exchange rule we get the plus/minus sign.

> 
- $n!$ nonezero terms.
- Each nonezero term has one entry from each row and each column.
- The column order indicates the plus/minus sign: from permutations to $I$.
{: .prompt-tip}

> "Big formula": \
\
$\displaystyle \det A = \sum (\det P)a_{1\alpha}a_{2\beta}\dots a_{n\omega} \hspace{1cm} P = (\alpha, \beta, \dots, \omega)$
{: .prompt-info}

#### Determinant by cofactors

Back with the 3 by 3 example above. When you separate out the factor $a_{11}$ or $a_{12}$ or $a_{13}$ that comes from the first row, you see linearity.

$$\displaystyle\begin{align}
\det A = 
& + a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}
\\
& - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}
\\
= & a_{11}(a_{22}a_{33} - a_{23}a_{33}) + a_{12}(a_{23}a_{31}- a_{21}a_{33}) + a_{13}(a_{21}a_{32} - a_{22}a_{31})
\end{align}$$

Those 3 quantities in parentheses are called `cofactors`. They are 2 by 2 determinants! To see this visually:

$$
\begin{vmatrix} 
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{vmatrix}
=\begin{vmatrix} 
a_{11} &  &  \\
 & a_{22} & a_{23} \\
 & a_{32} & a_{33}
\end{vmatrix}
+\begin{vmatrix} 
 & a_{12} &  \\
a_{21} &  & a_{23} \\
a_{31} &  & a_{33}
\end{vmatrix}
+\begin{vmatrix} 
 &  & a_{13} \\
a_{21} & a_{22} &  \\
a_{31} & a_{32} & 
\end{vmatrix}
$$

You cross out row $1$ and column $j$ to get a submatrix $M_{1j}$ of size $n-1$.

We have to watch signs: the sign pattern is plus-minus-plus-minus...Also, notice that whatever is possible for row 1 is possible for any other row. And from the transpose rule, it's also possible for columns!

$$A = \begin{bmatrix}
\bullet & \bullet & & \bullet \\
\bullet & \bullet & & \bullet \\
\bullet & \bullet & & \bullet \\
		&		 & a_{43}& \end{bmatrix}
\hspace{1cm}
\text{signs}(-1)^{i+j} = 
\begin{bmatrix}
+& - & + & - \\
-& + & - & + \\
+& - & + & - \\
-& + & - & +
\end{bmatrix}
$$

> The cofactor formula:\
\
$$\begin{align}
\det A = \space & a_{i1}C_{i1} + a_{i2}C_{i2} + \dots + a_{in}C_{in} & \hspace{1cm} \text{By rows}
\\
\text{or}\hspace{0.5cm} & a_{1j}C_{1j} + a_{2j}C_{2j} + \dots + a_{nj}C_{nj} & \hspace{1cm} \text{By columns}
\\
\text{Cofator}\hspace{0.5cm} &
C_{ij} = (-1)^{i+j} \det M^{ij}
\end{align}$$
{: .prompt-info}

### Inverses, Cramer's Rule

This section solves $Ax = b$ and finds $A^{-1}$ by algebra, not by elimination.

#### Cramer's rule

Cramer's rule solves $Ax = b$ by determinants. $x$ and $b$ don't have determinant, therefore giving birth to the idea:

$$\text{Key idea}\hspace{1cm}
\begin{bmatrix} && \\ & A & \\ && \end{bmatrix}
\begin{bmatrix}
x_{1} & 0 & 0 \\
x_{2} & 1 & 0 \\
x_{3} & 0 & 1 \end{bmatrix}
= \begin{bmatrix}
b_{1} & a_{12} & a_{13} \\
b_{2} & a_{22} & a_{23} \\
b_{3} & a_{32} & a_{33} \end{bmatrix}
= B_{1}
$$

Expand $x$ and $y$ to matrices: the 'new $x$' is $I$ with the first column replaced by $x$, giving a matrix with determinant $x_{1}$. Multiply it by $A$ and the first column is $Ax = b$ and other columns the same as $A$.

By product rule this gives:

$$\text{Product rule}\hspace{1cm}
\begin{align}
(\det A)(x_{1}) & = \det B_{1}
\\
x_{1} & = {\det B_{1} \over \det A}
\end{align}$$

$$\text{Same idea}\hspace{1cm}
\begin{bmatrix}
\vert & \vert & \vert \\
a_{1} & a_{2} & a_{3} \\
\vert & \vert & \vert \end{bmatrix}
\begin{bmatrix}
1 & x_{1} & 0 \\
0 & x_{2} & 0 \\
0 & x_{3} & 1 \end{bmatrix}
= \begin{bmatrix}
\vert & \vert & \vert \\
a_{1} & b & a_{3} \\
\vert & \vert & \vert \end{bmatrix}
= B_{2}
$$

> Cramer's rule: If $\det A \neq 0$, $Ax = b$ is solved by determinants:\
\
$$\displaystyle
x_{i} = {\det B_{i} \over \det A}$$\
\
$B_{i}$ has the $j^{th}$ column of $A$ replaced by $b$.
{: .prompt-info}

To solve a system, Cramer's rule involves a considerable amount of calculation. Normally we'll not consider this approach.

#### Inverses using determinants and cofactors

Cramer's rule casts insight onto inverses $AA^{-1} = I$: _**let $x$'s be columns of $A^{-1}$ and $b$'s columns of $I$**_. Now the right sides become:

$$\det B's \hspace{0.5cm}
\begin{vmatrix} 
1 & a_{12} & a_{13} \\
0 & a_{22} & a_{23} \\
0 & a_{32} & a_{33}
\end{vmatrix}\hspace{0.5cm}
\begin{vmatrix} 
a_{11} & 1 & a_{13} \\
a_{21} & 0 & a_{23} \\
a_{31} & 0 & a_{33}
\end{vmatrix}\hspace{0.5cm}
\begin{vmatrix} 
a_{11} & a_{12} & 1 \\
a_{21} & a_{22} & 0 \\
a_{31} & a_{32} & 0
\end{vmatrix}\hspace{0.5cm}\dots\hspace{0.5cm}
% \text{Cofactors of }A
$$

The $\vert B \vert$'s are exactly COFACTORS of $A$! Notice that the plus/minus signs are correct.

> 
$$\displaystyle\text{FORMULA FOR }A^{-1}\hspace{1cm}
(A^{-1})_{ij} = {C_{ji} \over \det A}
\hspace{0.5cm}\text{and}\hspace{0.5cm}
A^{-1} = {C^{T} \over \det A}$$
{: .prompt-info}

To visualize $A^{-1} = {C^{T} / \det A}$:

$$AC^{T} = (\det A)I \hspace{1cm}
\begin{bmatrix} 
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{bmatrix}
\begin{bmatrix} 
C_{11} & C_{21} & C_{31} \\
C_{12} & C_{22} & C_{32} \\
C_{13} & C_{23} & C_{33}
\end{bmatrix}
= \begin{bmatrix}
\det A & 0 & 0 \\
0 & \det A & 0 \\
0 & 0 & \det A \end{bmatrix}
$$

Each row of $A$ and column of $C^{T}$ are exactly factors and cofactors in the formula for $\det A$. So the main diagonal are $\det A$'s.

How to explain the 0's off the main diagonal? The rows of $A$ are multiplying cofactors from different rows.

$$\begin{align}
\text{row 2 of }A \text{ and }
 \text{row 1 of }C
 \\
a_{21}C_{11} + a_{22}C_{12} + a_{23}C_{13} = 0
\end{align}$$

View this in the point of cofactor rule: _this is the determinant of a new matrix with row 2 copied into row 1._ Then the matrix has determinant 0.

### Areas, Volumnes, Cross Product

#### Areas and volumes

> This section makes use of knowledge about [Linear Transformations](#6-linear-transformations).

Determinants can be used to describe a geometric property of linear transformations in $R^2$ and $R^3$.

If $S$ is a set in the domain of a linear transformation $T$, the determinant of $T$ is related to how the area/volume of $T(S)$ compares with that of $S$:

>
$$\begin{align}Area(T(S)) & = \vert\det{T}\vert\cdot Area(S)\\
Vol(T(S)) & = \vert\det{T}\vert\cdot Vol(S)\end{align}$$
{: .prompt-info}

Notice the area/volume is linear in terms of each basis vector. I think some intuition about linear transformations is enough for understanding this.

#### $*$ Cross product

> 
The cross product of $u = (u_1, u_2, u_3)$ and $v = (v_1, v_2, v_3)$ is the vector computed by the determinant:\
$$u \times v = \left|\begin{array} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ \hline
u_1 & u_2 & u_3 \\ v_1 & v_2 & v_3 \end{array}\right|$$\
where $i, j, k$ are the basis _vectors_.
{: .prompt-info}

Properties:
- $u\times v$ is perpendicular to $u$ and $v$.
- $u\times v = - v \times u$.
- $u \times u = 0$. (two equal rows)

## 5. Eigenvalues, Eigenvectors and Applications

### Eigenvalues and Eigenvectors

#### Definition

> 
$Ax = \lambda x$
{: .prompt-info}

Eigenvectors are those don't change direction when multiplied by $A$. Or in terms of linear transformation, eigenvectors stay in their own span after the linear transformation.

The number $\lambda$ is the `eigenvalue` of $A$. $x$ is an `eigenvector` of $A$.

Examples:
- When $\lambda = 0$, $Ax = 0x = 0$ means that $x$ is in the nullspace: **Singular matrices have** $\lambda = 0$.
- The eigenvalue of $I$ is 1. All vectors are eigenvectors of $I$.
- For projection matrices $P$, $1$ is one of its eigenvalues when $P \neq 0$. $0$ is when $P$ is not $R^{n}$. In fact the dimension of $\mathbf{C}(A)$ is the number of 1's, and $\mathbf{N}(A^{T})$ for 0's.
- For Markov matrices(each column has a sum of 1), $\lambda = 1$ always exists.
- **For symmetric matrices, eigenvectors are orthogonal.**
- Reflection matrices has $\lambda = 1$ and $-1$. $R = 2P - I$. The 1 is the symmtric axis.
- One of the eigenvectors of a rotation matrix is the axis of rotation (if exists), and that eigenvalue is 1.

> [Later section](#symmetric-s--qlambda-qt) will elaborate on the symmetric case.

> Orthogonal matrices have $\vert\lambda\vert = 1$.
{: .prompt-tip}

Proof: $Qx = \lambda x \longrightarrow \vert\vert Qx \vert\vert = \vert\vert \lambda x \vert\vert \longrightarrow \vert\vert x \vert\vert = \vert\lambda\vert \cdot \vert\vert x \vert\vert \longleftrightarrow \vert\vert \lambda \vert\vert = 1$.

Notice its the _length_ of $\lambda$. Generally a column of $Q$ does not sum to 1.

Therefore, **symmetric orthogonal matrices have** $\lambda = \pm 1$. Also, $(A + I) / 2$ is a projection matrix --- check out the eigenvalues, or square it.

#### Calculating eigenvalues

$$\begin{align}
Ax = & \lambda x \\
(A - \lambda I)x = & 0 \hspace{0.5cm}\text{with}\hspace{0.5cm}x \neq 0 \\
\det (A - \lambda I) = & 0
\end{align}$$

Eigenvectors $x$ must not be 0, therefore $A - \lambda I$ must be singular. **The eigenvectors make up the nullspace of** $A - \lambda I$.

Then, for each $\lambda$, solve $(A - \lambda I)x = 0$ or $Ax = \lambda x$ to find an eigenvector $x$.

> To compute eigenvalues of an $n$ by $n$ matrix: 
1. Compute $\det (A - \lambda I)$.
2. Find the roots of $\det (A - \lambda I) = 0$, a polynomial of degree $n$.
3. For each $\lambda$, solve $(A - \lambda I)x = 0$ to find an eigenvector $x$.
{: .prompt-tip}

The degree of this polynomial confirms that:

> A $n$ by $n$ matrix has $n$ eigenvalues.
{: .prompt-tip} 

#### Determinant and trace

By observing the polynomial $\det (A - \lambda I) = 0$, we can conclude that:

> 
- $\det A = \prod \lambda_{i}$.
- $trace = \sum \lambda_{i}$.
{: .prompt-tip}

> 
- Triangular matrices has $\lambda$'s on the diagonal.
{: .prompt-tip}

> Those checks are very useful. For example when you have a singular 2 by 2 matrix, you instantly know one $\lambda = 0$, then from the trace you get the other one.

> Elimination does not preserve eigenvalues and eigenvectors.
{: .prompt-warning}

Because while the determinant doesn't change during elimination, the trace does.

$$\text{Proof}\hspace{1cm}
\begin{align} A = & LDU \\
\text{with}\hspace{0.5cm} Ax = \lambda x \hspace{0.5cm}\text{and}&\hspace{0.5cm} Ux = \lambda x
\\
Ax = & LDUx \\
\lambda x = & LD\lambda x \\
LD = & I
\end{align}$$

#### Eigenvalues of $AB$ and $A+B$

> $\lambda_{A}\lambda_{B} \neq \lambda_{AB} \hspace{1cm}\text{and}\hspace{1cm} \lambda_{A} + \lambda_{B} \neq \lambda_{A + B}$
{: .prompt-warning}

$$\text{False proof}\hspace{1cm}
ABx = A\lambda_{B}x = \lambda_{B}Ax = \lambda_{B}\lambda_{A}x$$

This mistake uses an assumption: $A$ and $B$ share the same eigenvector $x$. Which they usually don't.

Same reason for $A+B$. Here is an counterexample:

$$\begin{align}
A = &\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}&
\hspace{0.5cm}
B = &\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}&
\hspace{0.5cm}\text{then}\hspace{0.5cm}
AB = &\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}&
\hspace{0.5cm}
A + B = &\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
\\
\lambda_{A} = & 0&
\lambda_{B} = & 0&
\lambda_{AB} = & 1,\space 0&
\lambda_{A + B} = & 1,\space -1
\end{align}$$

> $A$ and $B$ share the same $n$ independent eigenvectors if and only if $AB = BA$.
{: .prompt-tip}

However there are cases where the multiplication of $\lambda$ comes in handy.
- For projection $P$: $P^2 = P \longrightarrow \lambda^2 = \lambda \longleftrightarrow \lambda = 1 \text{ or } 0$.

#### $*$ Estimating eigenvalues

> Gershgorin circles:\
$\displaystyle\vert a_{ii} - \lambda \vert \le \sum_{j \neq i} \vert a_{ij} \vert  = R_{i}$
{: .prompt-tip}

Every $\lambda$ is 'near' at least one of the entries $a_{ii}$ on the diagonal. Every $\lambda$ is in the circle around one or more diagonal entries.

The reasoning is simple: $A - \lambda I$ is not invertible, then $A - \lambda I$ can't be _diagonally dominant_ (see the last section of Elimination). There must be one $\vert a_{ii} - \lambda \vert \le R_{i}$

#### Decomposition of input $x$

For every $x$ in the subspace spanned by eigenvectors of $A$, x can be factored into:

$$x = Xc = c_{1}x_{1} + \cdots + c_{n}x_{n}$$

This will prove helpful for [Difference Matrices](#discrete-stochastic-systems-difference-equations).

### Diagonalize $A = X\Lambda X^{-1}$

#### Diagonalization and its existence

> For diagonalizable $A$:\
$AX = X\Lambda \hspace{0.5cm}\longleftrightarrow \hspace{0.5cm} X^{-1}AX = \Lambda \hspace{0.5cm}\text{or}\hspace{0.5cm} A = X\Lambda X^{-1}$
{: .prompt-info}

> A is diagonalizable when: 
1. $A$ is $n$ by $n$.
2. $A$ has $n$ independent eigenvectors.
{: .prompt-info}

$$\Lambda = \begin{bmatrix} \lambda_{1} & & \\ & \ddots & \\ & & \lambda_{n}\end{bmatrix}$$ and $X = \begin{bmatrix} x_1 & \cdots & x_n\end{bmatrix}$ has columns of corresponding eigenvectors. This is impossible with dependent eigenvectors because $X$ would have no inverse.

$$\begin{align}
A\text{ times }X\hspace{1cm}&
AX = A\begin{bmatrix} \vert & & \vert \\ x_{1} & \cdots & x_{n} \\ \vert & & \vert \end{bmatrix}
= \begin{bmatrix} \vert & & \vert \\ \lambda_{1}x_{1} & \cdots & \lambda_{n}x_{n} \\ \vert & & \vert \end{bmatrix}
\\
X\text{ times }\Lambda\hspace{1cm}&
\begin{bmatrix} \vert & & \vert \\ \lambda_{1}x_{1} & \cdots & \lambda_{n}x_{n} \\ \vert & & \vert \end{bmatrix}
= \begin{bmatrix} \vert & & \vert \\ x_{1} & \cdots & x_{n} \\ \vert & & \vert \end{bmatrix}
\begin{bmatrix} \lambda_{1} & & \\ & \ddots & \\ & & \lambda_{n}\end{bmatrix} = X\Lambda
\end{align}$$

> There is no connection between invertibility and diagonalizability:
- Invertible $\longleftrightarrow$ $\lambda \neq 0$.
- Diagonalizable $\longleftrightarrow$ $n$ independent eigenvectors.
{: .prompt-warning}

> 
- Any matrix with no repeated $\lambda$'s can be diagonalized, and NOT vice versa.
- Eigenvectors that come from distinct $\lambda$'s are linearly independent.
{: .prompt-tip}

#### $*$ Non-diagonalizable matrices

$\lambda$ may be a simple eigenvalue or a multiple(repeated) eigenvalue, and we want to know its _multiplicity_. Always $GM \leq AM$ for each $\lambda$:

1. (Geometric multiplicity) $GM$ counts the **independent** eigenvectors for $\lambda$. Then $GM = \dim \mathbf{N}(A - \lambda I)$.
2. (Algebraic multiplicity) $AM$ counts the repetitions of $\lambda$ from the $n$ roots of $\det (A - \lambda I) = 0$. 

Example:

$$\begin{align}
A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} 
&\hspace{0.5cm}\text{has}\hspace{0.5cm}&\det (A - \lambda I)  
 = \begin{vmatrix} -\lambda & 1 \\ 0 & -\lambda \end{vmatrix} = \lambda^{2}.
\\
AM = 2&& \lambda = 0,\space 0
\\
GM = 1&& \text{but 1 eigenvector}
\end{align}$$

The shortage of eigenvectors when $GM$ is below $AM$ measn $A$ is not diagonalizable.

> 
- $GM < AM \longleftrightarrow$ non-diagonalizable $A$.
- Every $GM = AM \longleftrightarrow$ diagonalizable $A$.\
A weaker statement: **no repeated eigenvalues means diagonalizable**.
{: .prompt-info}

A classic case where repeated eigenvalues but still diagonalizable: $I$.

#### Powers of $A$

> 
$$A^{k} = (X\Lambda X^{-1})(X\Lambda X^{-1})\dots(X\Lambda X^{-1}) = X\Lambda^{k} X^{-1}$$
{: .prompt-info}

For diagonalizable $A$ the powers will be easy to compute.

For difference equation $u_{k+1} = Au_{k}$, the solution is:

$$\begin{align}
\text{Solution}\hspace{0.5cm}
u_k & = A^{k}u_{0} = X\Lambda^{k}X^{-1}u_{0}  \\
	& = c_{1}(\lambda_{1})^{k}x_{1} + \cdots + c_{n}(\lambda_{n})^{k}x_{n} \\
\text{where}\hspace{0.5cm}
 u_{0} & = Xc = c_{1}x_{1} + \cdots + c_{n}x_{n}
\end{align}$$

This process has a `steady state` when $A^{k}$ has a limit, which only allows $\lambda = 1$ and other $\vert\lambda\vert < 1$.

### Similar Matrices, Jordan Form

#### Similarity

> Two matrices $A$ and $B$ are `similar` if $B = MAM^{-1}$.
{: .prompt-info}

Fix the eigenvalues in $\Lambda$, and change the eigenvectors in $X$, we get a whole family of matrices. This family can be represented by a diagonal matrix (or nearly diagonal, which will be explained later).

> Similar matirces have same eigenvalues.
{: .prompt-tip}

Proof: $Ax = \lambda x$ , then $MAM^{-1}$ has the same $\lambda$ as $A$:

$$\text{Same }\lambda\hspace{1cm}
(MAM^{-1})(Mx) = MAx = \lambda (Mx)$$

A slightly different proof:

$$\begin{align}
A(MM^{-1})x & = \lambda x \\
(M^{-1}AM)M^{-1}x & = \lambda M^{-1}x \\
B(M^{-1}x) & = \lambda (M^{-1}x) 
\end{align}$$

> Same eigenvalues $\neq$ similar matrices.
{: .prompt-tip}

If $A$ has repeated eigenvalues, it may not be diagonalizable. 

Example:

$$\begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix} \hspace{0.5cm}\text{and}\hspace{0.5cm} \begin{bmatrix} 4 & 1 \\ 0 & 4 \end{bmatrix} \hspace{0.5cm}\text{both have}\hspace{0.5cm} \lambda_1 = \lambda_2 = 4$$

But they come from two different families of matrices. Notice that $$\begin{bmatrix} 4 & 1 \\ 0 & 4 \end{bmatrix}$$ cannot even be diagonalized. $$\begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}$$ is the only member of the other family, because:

$$M^{-1}\begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}M = 4M^{-1}M = \begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}$$

We can find the family represented by $$\begin{bmatrix} 4 & 1 \\ 0 & 4 \end{bmatrix}$$:

$$\begin{bmatrix}4 & 0 \\ a & 4 \end{bmatrix}
\hspace{0.5cm}\text{and}\hspace{0.5cm}
\begin{bmatrix}4 & a \\ 0 & 4 \end{bmatrix}
\hspace{0.5cm}\text{and}\hspace{0.5cm}
\begin{bmatrix}a & b \\
(8a - a^2 -16)/b & 8 - a \end{bmatrix}
$$

They are all nondiagonalizable, if they weren't they would be similar to $$\begin{bmatrix} 4 & 0 \\ 0 & 4 \end{bmatrix}$$.

> 
- Same unrepeated eigenvalues $\longrightarrow$ diagonalizable $\longrightarrow$ similar matrices.
- Similarity $\neq$ diagonalizability.
{: .prompt-tip}

$$\begin{align}
A = X\Lambda X^{-1} 
\hspace{0.5cm}&\text{and}\hspace{0.5cm}
B = Y\Lambda Y^{-1}\\
(YX^{-1})A(XY^{-1}) = Y\Lambda Y^{-1} = B 
\hspace{0.5cm}&\text{and}\hspace{0.5cm}
YX^{-1} = (XY^{-1})^{-1}
\end{align}$$

#### Jordan form

Camille Jordan found a way to choose a _"most diagonal"_ representative from each family of similar matrices.

> `Jordan form`: Every square $A$ is similar to a Jordan matrix $J$, with Jordan blocks on the diagonal:\
$$J = \begin{bmatrix}
J_1 & 0 & \cdots & 0 \\
0 & J_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & J_d \end{bmatrix}
$$
{: .prompt-info}

> A `Jordan block` $J_i$ has a repeated eigenvalue $\lambda_i$ on the diagonal, 0's below and 1's on the upper diagonal:\
$$J_i = \begin{bmatrix}
\lambda_i & 1 & 0 & \cdots & 0 \\
0 & \lambda_i & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & \lambda_i & 1 \\
0 & 0 & \cdots & 0 & \lambda_i \end{bmatrix}$$
{: .prompt-info}

Suppose a $n \times n$ matrix has $s \le n$ independent eigenvectors, then Jordan form has:

> 
- $s$ square blocks along the diagonal.
- Each block has one eigenvalue $\lambda$, one eigenvector, and 1's above.
{: .prompt-tip}

The Jordan form of a matrix is unique. If two matrices have different Jordan forms, they cannot be similar.

> Example:

$$A = \begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \end{bmatrix}
\hspace{1cm} B = \begin{bmatrix}
0 & 1 & 7 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \end{bmatrix}
\hspace{1cm} C = \begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \end{bmatrix}$$

They all have eigenvalues 0's. 

$A$ has $rank = 2$ so $\dim N(A) = 2$. It has 2 independent eigenvectors and 2 'missing' eigenvectors. $A$ is the jordan normal form of the family. It has a 1 above the diagonal for every missing eigenvector, and the rest are 0.

$B$ has the same nullspace and same eigenvectors as $A$. It's similar to $A$ but not as nice. 

Now consider $C$. It's also of rank 2 and 2 eigenvectors, but not similar to $A$. We can see this by breaking the matrices into Jordan blocks:

$$\text{Jordan blocks}\hspace{1cm}
A = \left[\begin{array}{c c c | c}
0 & 1 & 0 & 0 \\ 
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\ \hline
0 & 0 & 0 & 0 \end{array}\right]
\hspace{1cm}C = \left[\begin{array}{c c | c c}
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\ \hline
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \end{array}\right]
$$

> Example over.

Two matrices may have the same eigenvalues and the same number of eigenvectors. But if their Jordan blocks are different sizes, they cannot be similar.

#### Similarity $\leftrightarrow$ same transformation

Similar matrices can be viewd as the same linear transfomation in different coordinate systems. See [here](#diagonal-transformations).

### Discrete Stochastic Systems: Difference Equations

#### Fibonacci

$$\text{Fibbonacci}\hspace{1cm} F_{k+2} = F_{k+1} + F_{k}$$

Problem: Find $F_{k}$ in terms of $k$.

Solution:

This is a second-order equation. However, we could still transform it into a first-order problem.

- **STEP 0** : Rewrite the equation into a  difference matrix.

$$\text{Let } u_k = \begin{bmatrix} F_{k+1} \\ F_{k}\end{bmatrix}. \text{ Then }
\begin{align}
F_{k+2} & = F_{k+1} + F_{k} \\
F_{k+1} & = F_{k+1}
\end{align}
\text{ is }
u_{k+1} = \begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix}u_k
$$

Eigenvectors are perfect for describing changes over time.

- **STEP 1**: Compute eigenvalues and eigenvectors.

$$A - \lambda I = \begin{bmatrix} 1 - \lambda & 1 \\ 1 & -\lambda \end{bmatrix}
\hspace{0.5cm}\longrightarrow\hspace{0.5cm}
\det (A - \lambda I) = \lambda^{2} - \lambda - 1
$$

$$\begin{align}
\text{Eigenvalues}\hspace{1cm}
&\lambda_1 = {1 + \sqrt{5} \over 2} \approx 1.618
&\hspace{0.5cm}\text{and}\hspace{0.5cm}
&\lambda_2 = {1 - \sqrt{5} \over 2} \approx -.618
\\
\text{Eigenvectors}\hspace{1cm}
&x_1 = (\lambda_1, 1)
&\hspace{0.5cm}\text{and}\hspace{0.5cm}
&x_2 = (\lambda_2, 1)
\end{align}$$

- **STEP 2**: Find the combination $u_0 = Xc$.

$$\begin{bmatrix} 1 \\ 0 \end{bmatrix}
= {1 \over \lambda_{1} - \lambda_{2}}\left(\begin{bmatrix} \lambda_1 \\ 1 \end{bmatrix} - \begin{bmatrix} \lambda_2 \\ 1\end{bmatrix}\right)
\hspace{1cm}\text{or}\hspace{1cm}
u_0 = {x_1 - x_2 \over \lambda_1 - \lambda_2}
$$

- **STEP 3**: Multiply by $(\lambda_{i})^{k}$.

$$u_k = {(\lambda_1)^{k}x_1 - (\lambda_2)^{k}x_{2} \over \lambda_1 - \lambda_2}
$$

As $k$ grows large, $(\lambda_2)^{k} \rightarrow 0$, then $F_k$ is the integer closest to $\displaystyle{1 \over \sqrt{5}}\left({1 + \sqrt{5} \over 2}\right)^{k}$.

#### Markov

**What makes a Markov matrix?**

Every column vector is a _probability vector_:
1. For evey $i,\space j$: $a_{ij} > 0$: Positive entries.
2. For every $j$: $\sum a_{ij} = 1$: Column entries sum to 1.

Markov matrix is to describe how Markov chain changes over time.

**Two Facts are immediate:**
1. Because of 1: $u_k = A^k u_0 > 0$ when $u_0 > 0$.
2. Because of 2: Multiplying a vector $u$ by Markov matrix $Au$ keeps the sum of its components.

**Key Feature 1**: A Markov matrix has one $\lambda = 1$. 

Simple reasoning: Every column of a Markov matrix has a sum of 1, subtracting 1 on the diagonal, the sum is 0 -- -- All rows add to 0, therefore they are dependent.

> Example: 

The _steady state_ for this Markov matrix is the eigenvector with $\lambda = 1$.

$$A = 
\begin{bmatrix} .8 & .3 \\ .2 & .7 \end{bmatrix}
\hspace{0.5cm}\text{has}\hspace{0.5cm}
\begin{align}
\lambda_1 & = 1 & \lambda_2 &= .5 \\
x_1 & = (3, 2) & x_2 &= (1, -1)
\end{align}
$$

$$
u_{\infty} = A^{\infty}u_{0} = c_{1}(\lambda_{1})^{\infty}x_1 + c_{2}(\lambda_2)^{\infty}x_2 = c_{1}x_{1}
$$

> Example over.

**Key Feature 2**: A Markov matrix has $\vert \lambda \vert_{max} = 1$.

Otherwise $A^{k}u_{0}$ can't keep the sum of $u_0$'s components -- -- the result would blow up.

However, this doesn't mean a Markov matrix always has a steady state: it could have a $\lambda = - 1$, that eigenvector in every step has a coefficient of either 1 or $-1$ -- -- no steady state.

**Conclusion**: A Markov matrix has steady state if no $\lambda = -1$. The steady state is the eigenvector(s) with $\lambda = 1$.

### Differential Equations

This section describes a continuous analogous of the diffrence equations studied earlier.

#### It's a linear system

System of differential equations:

$$\begin{matrix}
x'_1 & = & a_{11}x_1 & + & \cdots & + & a_{1n}x_n \\
x'_2 & = & a_{21}x_1 & + & \cdots & + & a_{2n}x_n \\
	& \vdots & \\
x'_n & = & a_{n1}x_1 & + & \cdots & + & a_{nn}x_n
\end{matrix}$$

$$x_1, \dots, x_n$$ are differentiable funcitons of $t$, with derivatives $$x'_1, \dots, x'_n$$. The $$a_{ij}$$ are constants. So write the system as a matrix differential equation:

$$\begin{equation}\label{eq1} x'(t) = Ax(t)\end{equation}$$

where

$$x(t) = \begin{bmatrix} x_1(t) \\ \vdots \\ x_n(t) \end{bmatrix} \hspace{1cm}
x'(t) = \begin{bmatrix} x'_1(t) \\ \vdots \\ x'_n(t) \end{bmatrix} \hspace{1cm}
A = \begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{n1} & \cdots & a_{nn} \end{bmatrix}
$$

A solution of this equation is a vector-valued function that satisfies the equation for all $t$ in some interval of real numbers, such as $t \ge 0$.

This equation is _linear_ because both differentiation and matrix multiplication are linear operators:

$$\begin{align}
(cu + dv)' & = cu' + dv' \\
		& = cAu + dAv = A(cu + dv)
\end{align}$$

Also, the identically zero function is trivially a solution. So the set of all solutions is a _subspace_ of the set of all continuous functions with values in $R^n$.

#### In search of the solutions

There always exists a _fundamental set of solutions_ to $x'(t) = Ax(t)$. It forms a _basis_ for the solutions in an $n$-dimensional vector space.

When $A$ is diagonal, the solutions are produced by elementary calculus. For example:

$$\begin{bmatrix} x_1'(t) \\ x_2'(t) \end{bmatrix}
= \begin{bmatrix} 3 & \\ & 5 \end{bmatrix}
\begin{bmatrix}x_1(t) \\ x_2(t)\end{bmatrix}$$

Then the solution is:

$$\begin{array}{cccr}
x_1'(t) & & = & 3x_1(t) \\
& x_2'(t) & = & -5x_2(t) 
\end{array}$$

From calculus:

$$\begin{bmatrix} x_1(t) \\ x_2(t) \end{bmatrix}
= \begin{bmatrix} c_1e^{3t} \\ c_2e^{-5t} \end{bmatrix}
= c_1\begin{bmatrix} 1 \\ 0 \end{bmatrix} e^{3t}
	+ c_2\begin{bmatrix} 0 \\ 1 \end{bmatrix} e^{-5t}
$$

The system is said to be _decoupled_ because each derivative of a function depends only on the function itself.

- This example suggests that for the general equation $$x' = Ax$$, a solution might be a _linear combination_ of functions of the form

$$x(t) = ve^{\lambda t}$$

for some scalar $\lambda$ and some fixed nonzero vector $v$. Notice that

$$\begin{array}{rrr}
x'(t) & = & \lambda ve^{\lambda t} \\
Ax(t) & = & Ave^{\lambda t}\end{array}$$

Since $e^{\lambda t} \neq 0$, $x'(t) = Ax(t)$ if and only if $\lambda v = Av$, or to say, $\lambda$ is an eigenvalue of $A$ and $v$ is that eigenvector. They form the `eigenfunctions` of the differential equation.

#### Decoupling the system

> This section uses some terms about _linear transformations_, but not important.

When $A$ is diagonalizable, suppose the eigenfunctions for $A$ are:

$$\text{Independent }v\hspace{1cm}
v_1e^{\lambda_1 t}, \dots, v_ne^{\lambda_n t}$$

Diagonalize $A = P\Lambda P^{-1}$ with $$P = \begin{bmatrix} v_1 & \cdots & v_n \end{bmatrix}$$ (I use $P$ instead of $X$ for aesthetic reasons). Make a _change of variable_, defining a new function $y$ by 

$$y(t) = P^{-1}x(t) \longleftrightarrow x(t) = Py(t)$$

It says that $y(t)$ is the _coordinate vector_ of $x(t)$ relative to the eigenvector basis. Substitute into the equation $x' = Ax$:

$$\begin{array}{rll}
{d \over dt}(Py) & = & A(Py) = (P\Lambda P^{-1})Py = P\Lambda y \\
& \downarrow & \\
Py' & = & P\Lambda y \\
& \downarrow & \\
y'& = & \Lambda y
\end{array}$$

This change of variable from $x$ to $y$ has _decoupled_ the system. To visualize this:

$$
\begin{bmatrix} y_1'(t) \\ y_2'(t) \\ \vdots \\ y_n'(t) \end{bmatrix}
= \begin{bmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\cdots & & \vdots & 0 \\
0 & \cdots & 0 & \lambda_n \end{bmatrix}
\begin{bmatrix} y_1(t) \\ y_2(t) \\ \vdots \\ y_n(t) \end{bmatrix}$$

Thus we have the solutions for $y' = \Lambda y$

$$y(t) = \begin{bmatrix} c_1e^{\lambda_1 t} \\ \vdots \\ c_ne^{\lambda_n t} \end{bmatrix} 
\hspace{0.5cm}\text{where}\hspace{0.5cm}
\begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}
 = y(0) = P^{-1}x(0) = P^{-1}x_0$$

Then solve $x$ for the original system:

$$\text{Solution}\hspace{0.5cm}
\begin{align}
x(t) & = Py(t) = \begin{bmatrix} v_1 & \cdots & v_n \end{bmatrix} y(t) \\
 & = c_1v_1e^{\lambda_1 t} + \cdots + c_nv_ne^{\lambda_n t}
\end{align}$$

### Symmetric $S = Q\Lambda Q^T$

#### Spectral theorem

> Symmetric matrices are always diagonalizable.
{: .prompt-tip}

> **Spectral Theorem**:\
$S = S^T \longleftrightarrow S = Q\Lambda Q^{-1} = Q\Lambda Q^T$ \
where $\Lambda$ is _real_ and $Q$ is _orthogonal_.
{: .prompt-info}

This section is to find two facts that spectral theorem relies on: 
1. $\lambda$'s of a symmetric matrix are all real; 
2. eigenvectors of a symmetric matrix are orthogonal and therefore can be chosen orthonormal. 

And find more about symmetric matrices.

#### Real eigenvalues

Suppose $Sx = \lambda x$. $x$ and $\lambda$ could be complex. $\lambda  = a + ib$ has conjugate $\bar{\lambda} = a - ib$ and $\lambda x$ has conjugate $\bar{\lambda}\bar{x}$.

> The reason that conjugating $\lambda x$ results in $\bar{\lambda}\bar{x}$ instead of something like $\bar{\lambda}x$ is because conjugates only change sign on the imaginary part.

Take conjugates of $Sx = \lambda x$. 

$$\begin{align}\begin{split}
Sx & =  \lambda x 
& \longleftrightarrow & S\bar{x} = \bar{\lambda}\bar{x} & \longleftrightarrow
& \bar{x}^{T}S & = & \bar{x}^{T}\bar{\lambda}
\\
& \downarrow 
&&&
& & \downarrow &
\\
\bar{x}^{T}Sx & =  \bar{x}^{T}\lambda x
&&&
& \bar{x}^{T}Sx & = & \bar{x}^{T}\bar{\lambda}x
\end{split}\end{align}$$

The left sides are the same, the right sides are $\bar{x}^{T}x$ which is the length squared of $x$ (this will be explained in [later section](#complex-vectors-and-matrices)) times a number. So the number must be equal: $\lambda = \bar{\lambda}$.

#### Orthogonal eigenvectors

Eigenvectors of a real symmetric matrix are orthogonal. Here we prove it in the case where eigenvalues are not repeated, and skip the proof for the repeated case. Nonetheless it's true in both cases.

For real matrix $A$ and vectors $x$ and $y$, we have inner product: 

$$<Ax, y> = <x, A^{T}y>$$

Now that $A^{T} = A$ and $x$, $y$ are its eigenvectors from eigenvalues $\lambda$, $\mu$. Then

$$\begin{align}
\lambda<x, y> & = <\lambda x, y> = <Ax, y> = <x, A^{T}y> = <x, Ay> = <x, \mu y> \\
& = \mu<x, y>\end{align}$$

Therefore $(\lambda - \mu)<x, y> = 0$. Since $\lambda \neq \mu$, $<x, y> = 0$.

> Matrix $A$ has orthogonal eigenvectors if $A^TA = AA^T$.
{: .prompt-info}

When $A$ commutes with its transpose, it has orthogonal eigenvectors. Families of matrices that pass this test:
- Symmetric $A = A^T$.
- Skew-symmetric $A = - A^T$.
- Orthogonal $A^TA = I$.

Omit the proof.

#### Projection onto eigenvectors

For $S = S^T$, we can write:

$$\begin{align}
S & = Q\Lambda Q^T  \\
  & = \begin{bmatrix} q_1 & q_2 & \cdots & q_n \end{bmatrix}
  \begin{bmatrix} \lambda_1 &&& \\ 
  				& \lambda_2 && \\
  				&& \ddots & \\
  				&&& \lambda_n \end{bmatrix}
  \begin{bmatrix} q_1^T \\ q_2^T \\ \vdots \\ q_n^T \end{bmatrix} \\
  & = \lambda_{1}q_{1}q_{1}^T + \lambda_{2}q_{2}q_{2}^T + \dots + \lambda_{n}q_{n}q_{n}^T
\end{align}$$

> Every symmetric matrix is the combination of orthogonal projection matrices onto its eigenvectors.
{: .prompt-tip}

To get a certain term of this orthogonal combination:

$$Sq_i = (\lambda_{1}q_{1}q_{1}^T + \dots + \lambda_{n}q_{n}q_{n}^T)q_i = \lambda_{i}q_{i}$$

Which is also the definition of eigenvectors.

#### Eigenvalues and pivots match signs

> For $S = S^T$, number of positive _pivots_ $=$ number of positive _eigenvalues_.
{: .prompt-tip}

> Proof is on book(5th edition) P342. Omitted here.

Because the eigenvalues of $A + bI$ are just $b$ more than the eigenvalues of $A$, this fact can help to find the range of eigenvalues of a symmetric matrix.

### Positive definiteness

#### Positive definite matrix

> A `postive definite matrix` has $A^T = A$ and:\
(These are equivalent. One is adequate)
1. $x^TAx > 0$ for all $x \neq 0$.
2. $\forall\lambda > 0$.
3. $\forall\det A_i > 0$. $A_i$ are $1 \times 1, 2 \times 2, \dots, n \times n$ submatrices in the upper-left corner. 
4. $\forall\text{pivot} > 0$.
{: .prompt-info}

Similarly we have:
- `Positive semi-definite` if $x^TAx \ge 0$.
- `Negative definite` if $x^TAx < 0$ for all $x \neq 0$.
- `Indefinite` if $x^TAx$ has both positive and negative values. 

> If $S$ and $T$ are positive definite, so is $S + T$.
{: .prompt-tip}

Because $x^T(S + T)x = x^TSx + x^TTx > 0$ (when $x \neq 0$).

> If columns of $A$ are independent, then $A^TA$ is positive definite.
{: .prompt-tip}

$x^T(A^TA)x = x^TA^TAx = (Ax)^T(Ax) \ge 0$. If columns are independent, this equals to 0 only when $x = 0$. If columns are dependent, $A^TA$ is positive semi-definite.

#### Quadratic form

A $2 \times 2$ example:

$$2 \times 2 \hspace{1cm}\begin{align}
x^T Ax & = \begin{bmatrix} x_1 & x_2 \end{bmatrix}
\begin{bmatrix} a & b \\ b & c \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\\
& = ax_1^2 + 2bx_1x_2 + cx_2^2
\end{align}$$

This is the `quadratic form` $Q(x)$ of $A$. To see this more clear, we can write it into squares:

$$ Q(x) = a(x_1 + {b \over a}x_2)^2 + {ac - b^2 \over a}x_2^2
$$

There is a deep **relation between quadratic form and elimination**:

$$\displaystyle A = \begin{bmatrix} a & b \\ b & c \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ {a \over b} & 1 \end{bmatrix}\begin{bmatrix} a & b \\ 0 & {ac - b^2 \over a} \end{bmatrix} = LU
$$

The multipliers and pivots are just in place! This is no accident.

#### Tests for a minimum

For $f(x)$, the test for a minimum is: $df/dx = 0$ and $d^2f/dx^2 > 0$. Quadratic form $F(x, y)$ produce a symmetric matrix which contains four second derivatives. 

$$\text{Hessian matrix}\hspace{1cm}
S_2 = \begin{bmatrix} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{bmatrix}
\hspace{1cm}S_3 = \begin{bmatrix}
f_{xx} & f_{xy} & f_{xz} \\
f_{yx} & f_{yy} & f_{yz} \\
f_{zx} & f_{zy} & f_{zz} \end{bmatrix}
$$

It's symmetric because $f_{xy} = f_{yx}$.

In quadratic form, first derivatives are 0 at (0, 0). Positive second derivatives change to positive definiteness.

> Quadratic form $F(x, y)$ has a minimum at $(0, 0)$ if $A$ is positive definite.
{: .prompt-tip}

![quadratic](p2.png)
_(a)positive definite, (b)positive semi-definite, (c)indefinite, (d)negative definite_

Examples:

$$\begin{bmatrix}2 & 6 \\ 6 & 7\end{bmatrix}$$ is not positive definite. It has quadratic form: 

$$f(x, y) = 2x^2 + 12xy + 7y^2 = 2(x + 3y)^2 - 11y^2$$

It does not have a minimum at $(0, 0)$.

$$\begin{bmatrix}2 & 6 \\ 6 & 20\end{bmatrix}$$ is positive definite. It has quadratic form: 

$$f(x, y) = 2x^2 + 12xy + 20y^2 = 2(x + 3y)^2 + 2y^2 \ge 0$$

It has a minimum at $(0, 0)$.

<!-- ![summary1-1](summary 1-1.png)
_(0, 0, 0) is the saddle point_

![summary1-2](summary 1-2.png) -->

#### Change of variable

Since $A$ is symmetric, a natural thought would be to diagonalize:

$$x^TAx = x^T(Q^T\Lambda Q)x = (Qx)^T\Lambda (Qx)$$

Let $y = Qx$, then $x^TAx = y^T\Lambda y$. _Diagonalization makes all the cross-product terms disappear_, leaving powers of the new variable.

> Principal axes theorem:\
For $n$ by $n$ symmetric $A$, there is an orthogonal change of variable $y = Qx$, such that $x^TAx$ transforms into $y^T\Lambda y$ with no cross-product term.
{: .prompt-info}

The columns of $Q$ are called the _principal axes_ of quadratic form $x^TAx$.

> Let $y = Px$ where $P$ is any invertible matrix, the new $y^TA'y$ does not change definiteness.
{: .prompt-tip}

#### Geometry

**The ellipse $ax^2 + 2bxy + cy^2 = 1$**

Think of a tilted ellipse $x^TAx = 1$ in the $xy$ plane. Here's an example:

$$\begin{align} 
x^TAx & = \begin{bmatrix} x & y \end{bmatrix}
\begin{bmatrix} 5 & 4 \\ 4 & 5 \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix} \\
& = 5x^2 + 8 xy + 5 y^2 = 1
\end{align}$$

$$A = Q\Lambda Q^T \hspace{1cm}
\begin{bmatrix} 5 & 4 \\ 4 & 5 \end{bmatrix}
= {1 \over \sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} 
\begin{bmatrix} 9 & \\ & 1 \end{bmatrix}
{1 \over \sqrt{2}}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} $$

$$\begin{align}\text{Sum of}\\\text{squares}\end{align}\hspace{1cm}
5x^2 + 8xy + 5y^2 = 9({x + y \over \sqrt{2}})^2 + 1({x - y \over \sqrt{2}})^2 = 1
$$

![P355](355.png)

The right side is the line-up:

$$\begin{align}\text{Line up}\hspace{1cm}
{x + y \over \sqrt{2}} = X \hspace{1cm}
{x - y \over \sqrt{2}} = Y \hspace{1cm}
9X^2 + Y^2 = 1
\end{align}$$

These pictures show the geometry behind the factorization $A = Q\Lambda Q^T$:
1. The tilted ellipse is associated with $A$. It's equation is $x^TAx = 1$.
2. The line-up ellipse is associated with $\Lambda$ in $X^T\Lambda X = 1$.
3. The rotation matrix that lines up the ellipse is the eigenvector matrix $Q$. 

**The axes of tilted ellipse point along the eigenvectors**. This explains why $A = Q\Lambda Q^T$ is called the _principal axis theorem_ --- it displays the axes, both directions and lengths.

<!-- > Thought: Does this have something to do with _change of basis_ ? -->

More to note:

- $A = I$ gives $x^2 + y^2 = 1$, which is a circle. 
- If one $\lambda < 0$, the graph changes to a _hyperbola_. The sum of squares becomes a _difference of squares_ : $9X^2 - Y^2 = 1$.
- For a negative definite matrix like $A = -I$, with all $\lambda < 0$, the graph of $-x^2 - y^2 = 1$ has no points.

![hyperbola](p3.png)

If $A$ is $n \times n$, $x^TAx = 1$ is an "ellipsoid" in $R^n$. Its axes are the eigenvectors of $A$.

#### Eigenvalues, eigenvectors

> This section is mainly for [SVD](#svd-a--usigma-vt), to see how eigenvectors and eigenvalues of $A^TA$ are constructed.

> Let $A = A^T$. Define $m$ and $M$:\
$$m = min\{x^TAx : \vert\vert x \vert\vert = 1\}, M = max\{x^TAx : \vert\vert x \vert\vert = 1\}$$\
Then $M$ is the greatest $\lambda_1$ of $A$ and $m$ is the least $\lambda$ of $A$. $x^TAx = M$ when $x = u_1$ is the corresponding unit eigenvector. $x^TAx = m$ when $x$ is the corresponding unit eigenvector.
{: .prompt-info}

Proof:

Apply change of variable:

$$x^TAx = y^T\Lambda y \hspace{0.5cm}\text{when}\hspace{0.5cm} y = Qx$$

Notice:

$$\vert\vert y \vert\vert = \vert\vert Qx \vert\vert = \vert\vert x \vert\vert = 1$$

To simplify notation, suppose $A$ is 3 by 3 with eigenvalues $a \ge b \ge c$:

$$\Lambda = \begin{bmatrix} a && \\ & b & \\ && c\end{bmatrix}$$

For any $y = (y_1, y_2, y_3)$, we have:

$$\begin{align}
ay_1^2 = ay_1^2\\
by_2^2 \le ay_2^2 \\
cy_3^2 \le ay_3^2
\end{align}$$

Obtain the inequalities:

$$\begin{align}
y^T\Lambda y & = ay_1^2 + by_2^2 + cy_3^2 \\
& \le a(y_1^2 + y_2^2 + y_3^2) \\
& = a\vert\vert y \vert\vert \\
& = a
\end{align}$$

$y^\Lambda y = a$ if and only if $y = e_1 = (1, 0, 0)$. So $M = a = e_1^T\Lambda e_1 = u_1^TAu_1$. A similar argument will prove the $m$ case.

> Continue from above:\
The maximum value of $x^TAx$ subject to the constraints:
$$x^Tx = 1, x^Tu_1 = 0$$\
is the second greatest eigenvalue $\lambda_2$, and the maximum is attained when $x = u_2$ is the corresponding eigenvector.
{: .prompt-info}

### Complex Vectors and Matrices

#### Conjugate transpose

For complex vectors, the 'conventional' dot product $x^{T}y$ is no longer good:

$$\text{Transpose}\hspace{1cm} z^{T}z = \begin{bmatrix} 1 & i \end{bmatrix}
\begin{bmatrix} 1 \\ i \end{bmatrix}
= 1 + (- 1) = 0$$

Instead, we take _conjugate transpose_ of $z$, known as `Hermitian` or 'adjoint': 

$$\text{Hermitian}\hspace{1cm}z^Hz = \bar{z}^{T}z = \begin{bmatrix} 1 & - i \end{bmatrix}
\begin{bmatrix} 1 \\ i \end{bmatrix}
= 1 + 1 = 2$$

This is the correct length squared for a complex vector. Properties are basically the same.

$$(Au)^{H}v = u^{H}(A^{H}v)$$

#### Hermitian matrices

Similarly: When you take transpose of a complex matrix, you take conjugate, too.

$$\text{Hermitian matrix}\hspace{1cm}
A^{H} = \bar{A}^T = A$$

Hermitian matrices share the same features with symmetric matrices:

1. Eigenvalues of a Hermitian matrix are real.
2. Eigenvectors of a Hermitian matrix are orthogonal.

The proofs are pretty much the same.

#### Unitary matrices

A unitary matrix $Q$ is a complex matrix that has orthonormal columns.

Just like real matrices, except it's Hermitian:

1. Q^{H}Q = I.
2. If $Q$ is squre, $Q^{H} = Q^{-1}$.
3. $\vert\vert Qz \vert\vert = \vert\vert z \vert\vert$. Therefore $Qz = \lambda z$ leads to $\vert\lambda\vert = 1$.

#### Roots of unity

The roots to $z^n = 1$ are $w = e^{i\theta} = e^{i2\pi / n}$. They are $n$ evenly spaces points on the unit circle in the complex plane, see the example of $w^8 = 1$.

![P445](445.png)

#### Conclusion: real versus complex

|REAL					|COMPLEX				|
|$x^2 = 1 \longleftrightarrow x = \pm 1$ |$z^n = 1 \longleftrightarrow z = 1, w, \dots, w^{n-1} (w = e^{2\pi i / n})$|
|Transpose: $(A^T)_ij = A_ij$ |Hermitian: $(A^H)_ij = \bar{A}_ij$|
|dot product: $x^{T}y$ |inner product: $u^{H}v$|
|orthogonality: $x^{T}y = 0$ |orthogonality: $u^{H}v = 0$|
|symmetric matrices: $A^{T} = A$ |Hermitian matrices: $A^H = A$|
|$S = Q\Lambda Q^{-1} = Q\Lambda Q^T$ (real $\Lambda$) |$S = U\Lambda U^{-1} = U\Lambda U^T$ (real $\Lambda$)|
|orthonormal columns: $Q^{T}Q = I$ |orthonormal columns: $U^{H}U = I$|
|orthogonal matrices: $Q^{T} = Q^{-1}$ |unitary matrices: $U^H = U^{-1}$|
|$\vert\vert Qx \vert\vert = \vert\vert x \vert\vert$ |$\vert\vert Uz \vert\vert = \vert\vert z \vert\vert$|

### $*$ FFT

Fourier-Transform is to represent $f$ as s sum of harmonics $c_{k}e^{ikx}$. Fast-Fourier-Transform is about multiplying quickly by Fourier matrices and its inverse: $F$ and $F^{-1}$. An ordinary product $Fc$ involves $n^2$ multiplications. FTT costs only ${1 \over 2}n\log n$.

#### The Fourier matrix

The Fourier matrix contains powers of 1 and $w$. The entry in row $j$, column $k$ is $w^{jk}$. (**zero-index** !!!)

$$F_n = \begin{bmatrix}
1 & 1 & 1 & \cdots & 1 \\
1 & w & w^2 & \cdots & w^{n-1} \\
1 & w^2 & w^4 & \cdots & w^{2(n-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & w^{n-1} & w^{2(n-1)} & \cdots & w^{(n-1)^2}
\end{bmatrix}
\hspace{1cm} w = e^{i2\pi / n}$$

> Many would use $\omega = e^{-2\pi i / N}$, which is $\bar{w}$. And $\bar{F}$ instead of this $F$.
{: .prompt-warning}

This matrix is symmetric, NOT Hermitian. ${1 \over n}F$ is unitary.

$$\begin{align}F^T & = F \\ F^{H}F & = n^{2}I \end{align} \longleftrightarrow F^{-1} = {1 \over n^2}\bar{F}$$

Example:

$$\begin{align}
\text{4-point}\\\text{Fourier}\\\text{series}
\end{align}\hspace{1cm}
\begin{bmatrix} y_0 \\ y_1 \\ y_2 \\y_3 \end{bmatrix} = Fc = \begin{bmatrix} 
1 & 1 & 1 & 1 \\
1 & w & w^2 & w^3 \\
1 & w^2 & w^4 & w^6 \\
1 & w^3 & w^6 & w^9 \end{bmatrix}
\begin{bmatrix} c_0 \\ c_1 \\ c_2 \\ c_3 \end{bmatrix}$$

The input is 4 complex coefficients $c_i$. The output is 4 function values $y_i$. The first output $y_0 = c_0 + c_1 + c_2 + c_3$ is the value of the Fourier series $\sum c_{k}e^{ikx}$ at $x = 0$. The second, third and fourth are ... at $x = 2\pi / 4$, $x = 4\pi / 4$ and $x = 6\pi / 4$. These are finite Fourier series that contain $n = 4$ terms and are evaluated at $n = 4$ equally spaced points. 

$$y_1 = c_0 + c_{1}e^{i2\pi / 4} + c_{2}e^{i4\pi / 4} + c_{3}e^{i6\pi / 4} = c_0 + c_1w +c_2w^2 + c_3w^3$$

#### The One FFT step

The key idea is to connect $F_n$ with $F_{n/2}$, the half-size. 

$$F_4 = \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & i & i^2 & i^3 \\
1 & i^2 & i^4 & i^6 \\
1 & i^3 & i^6 & i^9 \end{bmatrix}
\hspace{1cm}\text{and}\hspace{1cm}
F_2 = \begin{bmatrix}
1 & 1 \\ 1 & i^2 \end{bmatrix}
$$

Set $m = {1 \over 2}n$. The first $m$ and last $m$ components of $y = F_nc$ combine the half-size transforms $y' = F_mc'$ and $y'' = F_mc''$.

$$\begin{align}
y_j & = y'_j + (w_n)^j y''_j \\
y_{j+m} & = y'_j - (w_n)^j y''_j\\
j & = 0, 1, \dots, m - 1
\end{align}$$

This reconstructs $y$ into $y'$ and $y''$, by splitting $c$ into $c'$ and $c''$ and transform them by $F_m$. 

Those formulas come from separating $c_0 \dots , c_{n-1}$ into even $c_{2k}$ and odd $c_{2k+1}$ : $w$ is $w_n$.

$$\displaystyle y = Fc \hspace{1cm}
y_j = \sum_{0}^{n-1}w^{jk}c_k = \sum_{0}^{m-1}w^{2jk}c_{2k} + \sum_{0}^{m-1}w^{j(2k+1)}c_{2k+1}
$$

This is an **important fact**:

$$w_n^2 = w_{n/2} = w_m$$

$$\displaystyle\text{Rewrite}\hspace{1cm}
y_j = \sum (w_m)^{jk}c'_k + (w_n)^j\sum(w_m)^{jk}c''_k = y'_j + (w_n)^j y''_j
$$

For $j > m$, the minus sign comes from factoring out $(w_n)^m = -1$.

Here is the matrix form:

>
$$\displaystyle
	\begin{align}\text{One step}\\\text{of FFT}\end{align}\hspace{1cm}
F_{n} = \begin{bmatrix} I_{n / 2} & D_{n / 2} \\ I_{n / 2} & -D_{n / 2} \end{bmatrix}
\begin{bmatrix} F_{n / 2} & \\ & F_{n / 2} \end{bmatrix}
\begin{bmatrix} P \end{bmatrix}
$$
{: .prompt-info}

$P$ is the permutation matrix that permutes even _indices_ at the front and odd _indices_ at the end. \
$D_n$ is the diagonal matrix $diag(1, w, \dots , w_{n-1})$. 

Here is an example:

$$\begin{align}\text{Factors}\\\text{for FFT}\end{align}\hspace{1cm}
F_4 = \begin{bmatrix}
1 & & 1 & \\
& 1 & & i \\
1 & & -1 & \\
& 1 & & -i \end{bmatrix}
\begin{bmatrix}
1 & 1 & & \\
1 & i^2 & & \\
& & 1 & 1 \\
& & 1 & i^2 \end{bmatrix}
\begin{bmatrix}
1 & & & \\
& & 1 & \\
& 1 & & \\
& & & 1 \end{bmatrix}
$$

![P449](449.png)

#### Recursion

This step comes out natural: Keep going from $n/2$ to $n/4$, all the way down to $F_0$.

$$\begin{bmatrix}F_{n/2} & \\ & F_{n/2}\end{bmatrix} 
= \begin{bmatrix}
I & D & & \\
I & -D & & \\
& & I & D \\
& & I & -D \end{bmatrix}
\begin{bmatrix} F_{n/4} &&& \\ & F_{n/4} && \\ && F_{n/4} & \\ &&& F_{n/4} \end{bmatrix}
\left[\begin{array}
\text{pick} & 0, 4, 8, \dots \\
\text{pick} & 2, 6, 10, \dots \\
\text{pick} & 1, 5, 9, \dots \\
\text{pick} & 3, 7, 11, \dots \end{array}\right]
$$

### SVD: $A = U\Sigma V^T$

> Singular value decomposition:\
$$A = U\Sigma V^T$$
- _Diagonal_ $\Sigma$ has $r = rank A$ nonzero entries.
- _Orthogonal_ $U$ and $V$.
{: .prompt-info}

> $U$ and $V$ contain orthonormal bases from the four subspaces:
- $u_1, \dots, u_r$ from $C(A)$.
- $u_{r+1}, \dots, u_m$ from $N(A^T)$.
- $v_1, \dots, v_r$ from $C(A^T)$.
- $v_{r+1}, \dots, v_n$ from $N(A)$.
{: .prompt-tip}

$U$ and $V$ can be chosen in a different way. But as long as the two criterion are met, it's SVD.

> SVD is possible for **_any_** $m$ by $n$ matrix.
{: .prompt-tip}

#### Singular values, singular vectors

To begin with, we can think of $A$ as a mapping taking any vector $v$ from $C(A^T)$ to a vector $u$ in $C(A)$. So $Av = u$. This is a bijection, proved in [Section Orthogonality](#orthognality-of-the-four-subspaces).

To make it nice, we can always choose an _orthonormal basis_ for $C(A^T)$ of dimension $r$. For future purposes, they are selected a subset of _**eigenvectors of**_ $A^TA$. We need a scaler $\sigma_i$ to make $u_i$ _unit vector_:

$$\begin{align}\text{Length squared} \\ \text{of }Av_i \end{align}\hspace{1cm}
\vert\vert Av_i \vert\vert ^2 = v_i^T (A^T Av_i) = v_i^T(\lambda_i v_i) = \lambda_i = \sigma_i^2$$

$$\text{Unit length } u_i \hspace{1cm}Av_i = \sigma_i u_i$$

This had confused me for a while that the eigenvectors of $A^TA$ happens to be the _exact_ bases for $C(A^T)$ and $N(A)$. Here is the explanation. And it's the truly amazing idea that makes all these come together:

> Why this works:\
Suppose the _unit_ eigenvectors of $A^TA$ are $v_1, \dots, v_n$. Arranged in order $\lambda_1 \ge \dots \ge \lambda_n$. And $Av_i$ has $r$ nonzero terms.\
Then $v_1, \dots, v_r$ is an orthonormal basis for $C(A^T)$.
{: .prompt-info}

> Proof:
1. Since $A^TA$ is symmetric, $v_1, \dots, v_n$ is an _orthonormal basis_ for $R^n$.
3. Since $Av_1, \dots, Av_r \neq 0$, they are in $C(A)$.
4. For any $b = Ax$ in $C(A)$, write $x = c_1v_1 + \dots + c_nv_n$, and:\
$$\begin{align}
b & = Ax = c_1Av_1 + \dots + c_rAv_r + c_{r+1}Av_{r+1} + \dots + c_nAv_n \\
& = c_1Av_1 + \dots + c_rAv_r + 0 + \dots + 0
\end{align}$$
5. Thus $b$ is in $$span\{Av_1, \dots, Av_r\}$$, so they form a _basis_ for $C(A)$, and $r = rank A$.
6. Since $Av_{r + 1}, \dots, Av_n = 0$, and $v_{r + 1}, \dots, v_n$ are independent, they form an _orthonormal basis_ for $N(A)$.
7. Because $N(A)^\perp = C(A^T)$, $v_1, \dots, v_r$ form an _orthonormal basis_ for $C(A^T)$.
{: .prompt-tip}

It's such a nice coincidence that I find particularly satisfying.

> Actually this proof spoils almost everything: It nearly reveals the orthogonal bases for the four subspaces in SVD. Strang's book got around this by not making a rigorous proof, but it's more comfortable for me to have one here.

Put them all into matrices:

$$\begin{align}A\begin{bmatrix} v_1 & v_2 & \cdots & v_r \end{bmatrix}
& = \begin{bmatrix} \sigma_1u_1 & \sigma_2u_2 & \cdots & \sigma_ru_r \end{bmatrix} \\
& = \begin{bmatrix} u_1 & u_2 & \cdots & u_r \end{bmatrix}\begin{bmatrix}
\sigma_1 & & & \\
& \sigma_2 & & \\
& & \ddots & \\
& & & \sigma_r \end{bmatrix}\\
AV & = U\Sigma
\end{align}$$

Notice $u_1, \dots, u_r$ ***are automatically orthogonal:***

$$i \neq j\hspace{1cm}
u_i^Tu_j = (Av_i)^T(Av_j) = v_i^T(A^TA v_j) = v_i^T(\lambda_j v_j) = 0$$

Beautiful!

The next step is to include orthonormal bases of $N(A)$ and $N(A^T)$. Their corresponding $\sigma_i$ must be all 0's. The two new bases are automatically orthogonal to the two previous bases, respectively, because of orthogonality of the four subspaces!

So this concludes $AV = U\Sigma$:

$$\begin{align}
(m \times n)&(n \times n) \\
AV =& U\Sigma \\
(m \times m)&(m \times n) \end{align}
\hspace{1cm}
{\Large A}\begin{bmatrix} 
&&&& \\
&&&& \\
v_1 & \cdots & v_r & \cdots & v_n \\
&&&& \\
&&&& \end{bmatrix}
= \begin{bmatrix}
&&&& \\
&&&& \\
u_1 & \cdots & u_r & \cdots & u_m \\
&&&& \\
&&&& \end{bmatrix}
\begin{bmatrix}
\sigma_1 &&&& \\
& \ddots &&& \\
&& \sigma_r && \\
&&& 0 & \cdots
\end{bmatrix}
$$

![p4](p4.png)

Since $V$ is orthonormal, the decomposition of $A$ becomes easy: 

$$A = U\Sigma V^{-1} = U\Sigma V^T$$

$u_i$ are the `left singular vectors`, $\sigma$'s (including the 0's) are the `singular values`, and $v_i$(no transpose) are the `right singular vectors`.

![P392](392.png)
_linear transformations_

> [Supplementary reading](#eigenvalues-eigenvectors).

> 
- $A$ is invertible $\longleftrightarrow$ $A$ has no $\sigma = 0$.
- $rank(A) =$ number of $\sigma \neq 0$.
{: .prompt-tip}

> Every $A$ has a unique $\Sigma$, but not $U$ and $V$.
{: .prompt-tip}

#### Computing SVD

$$\begin{align}
A^TA & = (U\Sigma V^T)^T(U\Sigma V^T) = V\Sigma^T U^T U\Sigma V^T = V\Sigma^T\Sigma V^T \\
AA^T & = (U\Sigma V^T)(U\Sigma V^T)^T = U\Sigma V^T V\Sigma^T U^T = U\Sigma^T\Sigma U^T 
\end{align}$$

$A^TA$ and $AA^T$ are both positive (semi)definite. They have form $A = Q\Lambda Q^T$ and $\Lambda$ has non-negative entries $\sigma_i^2$. This will be the way we compute singular value decomposition:

> Compute singular value decomposition:
1. Find $m$ by $n$ $\Sigma$ from $\Lambda$ of $A^TA$ or $AA^T$ : $\sigma_i = \sqrt{\lambda_i}$ and the rest all 0. 
2. Find orthogonal $V$ from $A^TA = V\Sigma^T\Sigma V^T$.
3. Find orthogonal $U$ from $AA^T = U\Sigma^T\Sigma U^T$.
{: .prompt-tip}

Or you could compute $u_i = Av_i / \sigma_i$.

$\Sigma^T\Sigma$ is an $n$ by $n$ diagonal matrix. Its diagonal has $r$ nozero entries and $n - r$ 0's.

> The computation can often get messed up. Be sure to check the result.

SVD can also be viewed this way:

$$
A = U\Sigma V^T = \sigma_1u_1v_1^T + \cdots + \sigma_r u_r v_r^T$$

A is a combination of rank-1 matrices. This is used in image compression: The entries of $U$, $\Sigma$ and $V$ combined are a lot less than $A$. $\sigma_i$ are grey degrees. $u_i$ and $v_i$ could be encoded in advance --- then the volumne of an image is significantly reduced. Furthermore, we can sort the $\sigma$'s in descent order, and eliminate the smalls that are below a "threshold".

#### Pseudoinverse

When $r < n$ or $r < m$, $A$ has no two-sided inverse. But it has a _pseudoinverse_ $A^+$ with the same rank $r$.

$$\large\begin{cases}
A^+ u_i = {1 \over \sigma_i} v_i & i \le r \\
A^+ u_i = 0 	& i > r
\end{cases}$$

$$\large\text{Pseudoinverse}\hspace{1cm}A^+ = V\Sigma^+U^T$$

$$\sum_{m \times n} = 
\begin{bmatrix}
\sigma_1 &&&& \\
& \ddots &&& \\
&& \sigma_r && \\
&&& 0 & \cdots
\end{bmatrix}\hspace{1cm}
\sum_{n \times m} {\large\mathbf{^+}} =
\begin{bmatrix}
\sigma_1 &&& \\
& \ddots && \\
&& \sigma_r & \\
&&& 0\\
&&& \vdots
\end{bmatrix}$$

![P396](396.png)

$$A^+A = \Sigma^+\Sigma = \left[\begin{array}{c|c} \Large{I} & \\ \hline & 0 \end{array}\right]$$

$A^+A$ is _as close to_ $I$ as it could. It's the projection matrix.

$$\begin{align}
AA^+ = &\text{ projection matrix onto }C(A) \\
A^+A = &\text{ projection matrix onto }C(A^T)
\end{align}$$

This implies another way to approach pseudoinverse:

$$\text{Projection}\hspace{1cm}
\begin{align}
P &= A(A^TA)^{-1}A^T = AA^+_{\text{right}} \\
P &= A^T(AA^T)^{-1}A = A^+_{\text{left}}A
\end{align}
$$

## 6. Linear Transformations

> This part was mainly derived from materials _other than_ 18.06 and its textbook. The course laid too less importance on this subject than I supposed would suffice.\
If you are following the course and find some topics somehow opaque, this might be of help.
{: .prompt-warning}

### Transformations, Linearity

#### Definition

A `transformation` $T$ is a mapping of vectors: $v \longrightarrow T(v)$ from $R^n$ to $R^m$. $R^n$ is the `domain` of $T$. $R^m$ is the `codomain` of $T$. 

$T$ is _linear_ when:
1. $T(v + w) = T(v) + T(w)$.
2. $T(cv) = cT(v)$.

- Combining these we get: $T(cv + dw) = cT(v) + dT(w)$. 
- A linear transformation always has $T(0) = 0$. 
- Equally spaced points go to equally spaced points.

The subspace spanned by all $T(v)$ is the `range` of $T$.

> A problem:

Let $T(A) = A^T$, $A$ is $2 \times 2$. Then $T$ is linear. $T^{-1}$ is $T$ itself: $T^2 = I$. We can write the matrix of $T$ in such bases:

- Standard basis: 

$$v_1 = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}\hspace{0.5cm}v_2 = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}\hspace{0.5cm}v_3 = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}\hspace{0.5cm}v_4 = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$$

Then: 

$$T(v_1) = v_1 \hspace{0.5cm} T(v_2) = v_3 \hspace{0.5cm} T(v_3) = v_2 \hspace{0.5cm} T(v_4) = v_4$$

Therefore 

$$T = \begin{bmatrix}1 &&& \\ && 1 & \\ & 1 && \\ &&& 1 \end{bmatrix}$$

- An arbitary basis: 

$$w_1 = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}\hspace{0.5cm}w_2 = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}\hspace{0.5cm}w_4 = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}\hspace{0.5cm}w_1 = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$$

Then: 

$$T(w_1) = w_1\hspace{0.5cm}T(w_2) = w_2\hspace{0.5cm}T(w_3) = w_3\hspace{0.5cm}T(w_4) = - w_4$$

$$T' = \begin{bmatrix} 1 &&& \\ & 1 && \\ && 1 & \\ &&& -1 \end{bmatrix}$$

> Example over.

#### One-to-one and onto transformations

> $T: R^n \longrightarrow R^m$ is `one-to-one` if: for every $b$ in $R^m$, the equation $T(x) = b$ has _**at most one**_ solution $x$ in $R^n$.
{: .prompt-info}

Other equivalent statements for _one-to-one_:
- For every $b$ in $R^m$, the equation $T(x) = b$ has _zero or one_ solution $x$ in $R^n$.
- Different inputs of $T$ has different ouputs.
- $T(u) = T(v) \longleftrightarrow u = v$.

In matrix language, _one-to-one_ is equivalent to _**full column rank**_.

> $T: R^n \longrightarrow R^m$ is `onto` if: for every $b$ in $R^m$, the equation $T(x) = b$ has _**at least one**_ solution $x$ in $R^n$.
{: .prompt-info}

Other equivalent statements for _onto_:
- Range of $T$ equals codomain of $T$.
- Every vector in the codomain is the ouput of some input vector.

In matrix language, _onto_ is equivalent to _**full row rank**_.

> One-to-one and onto are the same for square matrices.
{: .prompt-tip}

#### Matrix multiplication

Matrices as functions: 

Matrix multiplicaiton on the left is equivalent to a linear transformation: $Ax$ maps $x$ in $R^n$ to $C(A)$ in $R^m$.

This follows the same rule as $(f \circ g)x = f(g(x))$: $(A \circ B)x = A(Bx) = ABx$. It explains why matrix multiplications are not commutative.

![p1](p1.png)

> A special case: The _dot product_ of any vector in $R^n$ with $a$ is a linear transformation, and that transformation (or matrix) is $a$ 'itself': $T = a^T$. 

### Coordianate Systems

#### $\mathcal{B}$-coordinates

For a given basis $\mathcal{B}$ in vector space $V$, there exists a unique representation for every $x$ in $V$.

> Let $$\mathcal{B} = \{b_1, \dots, b_n\}$$ be a basis for $V$, and $x$ is in $V$. The _coordinates of $x$ relative to the basis_ $\mathcal{B}$, or the `B-coordinates` of $x$, are the weights $c_1, \dots, c_n$ such that:\
$x = c_1b_1 + \cdots + c_nb_n$
{: .prompt-info}

Then 

$$[x]_\mathcal{B} = \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}$$

is the `coordinate vector` of $x$ (relative to $\mathcal{B}$). The mapping $x \mapsto [x]_\mathcal{B}$ is the `coordinate mapping` (determined by $\mathcal{B}$).

#### Coordinates In $R^n$

Write $\mathcal{B}$ in matrix form:

$$P_\mathcal{B} = \begin{bmatrix} b_1 & \cdots & b_n\end{bmatrix} $$

Then the vector equation

$$x = c_1b_1 + \cdots + c_nb_n$$

is equivalent to

$$x = P_\mathcal{B}[x]_\mathcal{B}$$

Call $P_\mathcal{B}$ the `change-of-coordinates matrix` from $\mathcal{B}$ to the standard basis in $R^n$.

Since the $b$'s are a basis, $P_\mathcal{B}$ is invertible. Then $P_\mathcal{B}^{-1}$ converts $x$ into its $\mathcal{B}$-coordinate vector:

$$P_\mathcal{B}^{-1}x = [x]_\mathcal{B}$$

> Generally, find the $\mathcal{B}$-coordinates of $x$ by _**row-reducing the augmented matrix:**_.\
$$\left[\begin{array}{c|c}\mathcal{B} & x\end{array}\right] \longrightarrow
\left[\begin{array}{c|c} I & [x]_ \mathcal{B}\end{array}\right]$$
{: .prompt-tip}

#### The Coordinate Mapping

> Let $\mathcal{B}$ be a basis for $V$, the coordinate mapping $$x \mapsto [x]_\mathcal{B}$$ is a _one-to-one_ linear transformation from $V$ _onto_ $R^n$.
{: .prompt-info}

This coordinate mapping is an _isomorphism_ from $V$ onto $R^n$ (_iso_ from the Greek for _the same_, and _morph_ for _form_ or _structure_). We also call it the _identity transformation_. 

> For more information, refer to book _Applications_ P222.

### Change of Basis

#### Change of basis in $V$

> 
- Let $$\mathcal{C} = \{b_1, \dots, b_n\}$$ and $$\mathcal{B} = \{c_1, \dots, c_n\}$$ be bases of a vector space $V$. Then there is a unique $n \times n$ matrix $\underset{\mathcal{C}\leftarrow\mathcal{B}}{P}$ such that:\
$$ [x]_\mathcal{C} = \underset{\mathcal{C}\leftarrow\mathcal{B}}{P}[x]_\mathcal{B} $$
- The columns of $\underset{\mathcal{C}\leftarrow\mathcal{B}}{P}$ are the $\mathcal{C}$-coordinate vectors of $b$'s:\
$$\underset{\mathcal{C}\leftarrow\mathcal{B}}{P} = 
{\Large{[}} \begin{array}{ccc} [b_1]_\mathcal{C} & \cdots & [b_n]_\mathcal{C} \end{array} {\Large]}$$
{: .prompt-info}

This is the change-of-coordinates matrix from $\mathcal{B}$ to $\mathcal{C}$, or call it the _change-of-basis_ matrix. Multiplication by $\underset{\mathcal{C}\leftarrow\mathcal{B}}{P}$ converts $\mathcal{B}$-coordinates to $\mathcal{C}$-coordinates.

![coordinate systems](app-P156.png)

Since $\underset{\mathcal{C}\leftarrow\mathcal{B}}{P}$ is evidently invertible, we have:

$$\left(\underset{\mathcal{C}\leftarrow\mathcal{B}}{P}\right)^{-1}[x]_\mathcal{C} = [x]_\mathcal{B}$$

Thus $(\underset{\mathcal{C}\leftarrow\mathcal{B}}{P})^{-1}$ is the matrix converting $\mathcal{C}$-coordinates to $\mathcal{B}$-coordinates.

$$\left(\underset{\mathcal{C}\leftarrow\mathcal{B}}{P}\right)^{-1} = \underset{\mathcal{B}\leftarrow\mathcal{C}}{P}$$

#### Change of basis in $R^n$

> An example:

$$\hspace{0.5cm}\begin{align}\text{Input}\\\text{basis}\end{align}\hspace{0.5cm}
\begin{bmatrix}b_1 & b_2\end{bmatrix} = \begin{bmatrix} 3 & 6 \\ 3 & 8 \end{bmatrix}$$
$$\hspace{0.5cm}\begin{align}\text{Output}\\\text{basis}\end{align}\hspace{0.5cm}
\begin{bmatrix}c_1 & c_2\end{bmatrix} = \begin{bmatrix}3 & 0 \\ 1 & 2\end{bmatrix}$$

$$\hspace{0.5cm}
\begin{align}\text{Change}\hspace{0.5cm}
& c_1 = 1b_1 + 1b_2 \\
\text{of basis}\hspace{0.5cm}
& c_2 = 2b_1 + 3b_2
\end{align}$$

The key is to write the input $\mathcal{B}$ in terms of output $\mathcal{C}$.

I added inputs(bottom) and outputs(left) to help understand ($e_i$ are the standard basis vectors):

$$\begin{align}\begin{split}
& \mathcal{B} & & P & = & & \mathcal{C} \\
\begin{array}{rr} e_1 \\ e_2 \end{array} &
\begin{bmatrix} 3 & 0 \\ 1 & 2 \end{bmatrix} &
\begin{array}{rr} b_1 \\ b_2 \end{array} &
\begin{bmatrix} 1 & 2 \\ 1 & 3 \end{bmatrix} &
= &
\begin{array}{rr} e_1 \\ e_2 \end{array} & 
\begin{bmatrix} 3 & 6 \\ 3 & 8 \end{bmatrix}
\\
&
\begin{array}{cc} b_1 & b_2 \end{array} &
&
\begin{array}{cc} c_1 & c_2 \end{array} &
&
&
\begin{array}{cc} c_1 & c_2 \end{array}
\end{split}\end{align}$$

> Example over.

Recall that for each $x$ in $R^n$:

$$P_\mathcal{B}[x]_\mathcal{B} = x \hspace{0.5cm}P_\mathcal{C}[x]_\mathcal{C} = x 
\hspace{0.5cm}\text{and}\hspace{0.5cm}[x]_\mathcal{C} = P_\mathcal{C}^{-1}x$$

Then:

$$[x]_\mathcal{C} = P_\mathcal{C}^{-1}P_\mathcal{B}[x]_\mathcal{B}$$

> 
Change-of-basis matrix: \
$$\underset{\mathcal{C}\leftarrow\mathcal{B}}{P} = P_\mathcal{C}^{-1}P_\mathcal{B}$$
{: .prompt-tip}

> A problem from the recitation video:

The basis for all polynomials in $x$ of degree $\le 2$ is $1, x, x^2$. A different basis $w_1, w_2, w_3$ has values at $x = -1, 0, 1$ as follows:

$$\begin{array}{c|ccc}
x & w_1 & w_2 & w_3 \\ \hline
-1 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \end{array}$$

Problem: Express $y(x) = -x + 5$ in $W$.

> Solution:

- General approach:

Solve each 

$$w_i = a_i + b_i x + c_i x^2$$

Then solve 

$$y(x) = \alpha w_1 + \beta w_2 + \gamma w_3$$

- Trick (without finding $w$'s explicitly):

Compute $y$ at $x = -1, 0, 1$:

$$\begin{array}{c|ccc|c}
x & w_1 & w_2 & w_3 & y\\ \hline
-1 & 1 & 0 & 0 & 6\\
0 & 0 & 1 & 0 & 5\\
1 & 0 & 0 & 1 & 4\end{array}$$

We have a linear system:

$$\begin{align}
y(-1) & = \alpha w_1(-1) + \beta w_2(-1) + \gamma w_3(-1) \\
y(0) & = \alpha w_1(0) + \beta w_2(0) + \gamma w_3(0) \\
y(1) & = \alpha w_1(1) + \beta w_2(1) + \gamma w_3(1)
\end{align}$$

The matrix for this system:

$$\begin{bmatrix} 1 && \\ & 1 & \\ && 1 \end{bmatrix}\begin{bmatrix}\alpha \\ \beta \\ \gamma \end{bmatrix} = \begin{bmatrix} 6 \\ 5 \\ 4 \end{bmatrix}$$

Then

$$\begin{bmatrix}\alpha \\ \beta \\ \gamma \end{bmatrix} = \begin{bmatrix} 6 \\ 5 \\ 4 \end{bmatrix}$$

> Example over.

#### Calculating change-of-basis matrix

Generally it's inefficient to calculate an inverse of a matrix then multiply to another matrix: $$P_\mathcal{C}^{-1}P_\mathcal{B}$$. Instead, take advantage of the _augmented matrix_:

> 
$$\text{Row Reduction}\hspace{1cm}
\left[\begin{array}{ccc|ccc} 
\vert & & \vert & \vert & & \vert \\
c_1 & \cdots & c_n & b_1 & \cdots & b_n \\
\vert & & \vert & \vert & & \vert \end{array}\right] \longrightarrow 
\left[\begin{array}{c|c} I & \underset{\mathcal{C}\leftarrow\mathcal{B}}{P}\end{array}\right]$$
{: .prompt-tip}

### Diagonalization, Similarity

Think about such transformations: $T = B_{\text{out}}^{-1}AB_{\text{in}}$. It's the "same" transformation $A$ in different bases. We've seen several examples:

1. Eigenvectors: $B_{\text{in}} = B_{\text{out}} = X$ : eigenvectors of $A$. Then $X^{-1}AX = \Lambda$.\
This requires $A$ to be a square matrix with $n$ independent eigenvectors --- diagonalizable. $\Lambda$ is the transformation in the 'eigenspace'.\
When $A$ is symmetric, $\Lambda$ describes how $A$ stretches its input: $\vert\vert Sx \vert\vert = \vert\vert\lambda x \vert\vert = \vert\lambda\vert$.
2. $B_{\text{in}} = V$ and $B_{\text{out}} = U$ : singular vectors of $A$. Then $U^{-1}AV = \text{ diagonal } \Sigma$.\
The SVD works for all matrices.
3. $B_{\text{in}} = B_{\text{out}} = \text{ generalized eigenvectors of } A$. Then $B^{-1}AB = J$: the Jordan form.

This section is to view the diagonalization by eigenvectors $A = X\Lambda X^{-1}$ in linear transforamtions, and see that the transformation $x \mapsto Ax$ is essentially _the same_ as the simple mapping $u \mapsto \Lambda u$, from a certain perspective.

#### The matrix of a linear transformation

Recall that any $T: R^n \mapsto R^m$ can be represented by left multiplication by a matrix $A$. This is the _standard matrix_ of $T$. Here we need a representation for any linear transformation (allowing non-standard bases) between two vector spaces.

Let $V$ be an $n$-dimensional vector space, $W$ be an $m$-dimensional vector space, and $T$ be any linear transformation from $V$ to $W$. To associate a matrix with $T$, choose (ordered) bases $\mathcal{B}$ for $V$ and $\mathcal{C}$ for $W$.

Given any $x$ in $V$, the coordinate vector $$[x]_\mathcal{B}$$ is in $R^n$, and the coordinate vector of its image, $${\Large [}T(x){\Large ]}_\mathcal{C}$$ is in $R^m$.

![lin. trans.](app-P291.png)

Find the connetion between $$[x]_\mathcal{B}$$ and $${\Large [}T(x){\Large ]}_\mathcal{C}$$: Let $$\{b_1, \dots, b_n\}$$ be the basis $$\mathcal{B}$$ for $$V$$. For $$x = r_1b_1 + \cdots + r_nb_n$$, we have $$[x]_\mathcal{B} = \begin{bmatrix} r_1 \\ \vdots \\ r_n \end{bmatrix}$$. Then:

$$T(x) = T(r_1b_1 + \cdots + r_nb_n) = r_1T(b_1) + \cdots + r_nT(b_n)$$

Since the coordinate mapping from $W$ to $R^n$ is linear:

$${\Large [}T(x){\Large ]}_\mathcal{C} = r_1{\Large [}T(b_1){\Large ]}_\mathcal{C} + 
\cdots + r_n{\Large [}T(b_n){\Large ]}_\mathcal{C}$$

Since $\mathcal{C}$-coordinate vectors are in $R^n$, this equation can be written as a matrix equation:

$${\Large [}T(x){\Large ]}_\mathcal{C} = M[x]_\mathcal{B}$$

where

$$M = \left[\begin{array}{ccc} {\Large [}T(b_1){\Large ]}_\mathcal{C} & \cdots & {\Large [}T(b_n){\Large ]}_\mathcal{C}\end{array}\right]$$

$M$ is called _the matrix for $T$ relative to the bases $\mathcal{B}$ and_ $\mathcal{C}$.

![matrix M](app-P291-2.png)

#### $T$ from $V$ to $V$ 

Commonly $W$ is the same as $V$, and $\mathcal{C}$ is the same as $\mathcal{B}$, here the matrix $M$ is called the matrix for $T$ relative to $\mathcal{B}$, or the `B-matrix` for $T$.

$$\mathcal{B}\text{-matrix for } T: V \mapsto V:\hspace{1cm}
{\Large [}T(x){\Large ]}_\mathcal{B} = {\Large [}T{\Large ]}_\mathcal{B}{\Large [}x{\Large ]}_\mathcal{B}$$

![B-matrix](app-P292.png)

> Example problem:

The mapping $$T: P_2 \mapsto P_2$$ defined by $$T(a_0 + a_1t + a_2t^2) = a_1 + 2a_2t$$ is a linear transfomation (the differentiation operetor).

a. Find the $\mathcal{B}$-matrix for $T$, when $\mathcal{B} = \{1, t, t^2 \}$.\
b. Verify that $${\Large [}T(p){\Large ]}_\mathcal{B} = {\Large [}T{\Large ]}_\mathcal{B}{\Large [}p{\Large ]}_\mathcal{B}$$ for each $p$ in $$P_2$$.

> Solution:

a. Compute the image for each $\mathcal{B}$-coordinate vector:

$$
T(1) = 0 \hspace{1cm}
T(t) = 1 \hspace{1cm}
T(t^2) = 2t
$$

Write them in $\mathcal{B}$-coordinate vectors, and put together as the $\mathcal{B}$-matrix for $T$:

$$
{\Large [}T(1){\Large ]}_\mathcal{B} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \hspace{0.5cm}
{\Large [}T(t){\Large ]}_\mathcal{B} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \hspace{0.5cm}
{\Large [}T(t^2){\Large ]}_\mathcal{B} = \begin{bmatrix} 0 \\ 2 \\ 0 \end{bmatrix} \longrightarrow
{\Large [}T{\Large ]}_\mathcal{B} = \begin{bmatrix}
0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0 \end{bmatrix}
$$

b. For a general $$p(t) = a_0 + a_1t + a_2t^2$$

$$\begin{align}
{\Large [}T(p){\Large ]}_\mathcal{B} & = \left[ a_1 + 2a_2t \right] 
= \begin{bmatrix} a_1 \\ 2a_2 \\ 0 \end{bmatrix} \\
& = \begin{bmatrix}
0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0 \end{bmatrix}
\begin{bmatrix} a_1 \\ a_1 \\ a_2 \end{bmatrix} = 
{\Large [}T{\Large ]}_\mathcal{B}{\Large [}p{\Large ]}_\mathcal{B}
\end{align}$$

![P293](app-P293.png)

> Example over.

#### Diagonal transformations

> Suppose $A = X\Lambda X^{-1}$. When $\mathcal{B}$ is the basis for $R^n$ formed from columns of $X$, then $\Lambda$ is the $\mathcal{B}$-matrix for the transformation $x \mapsto Ax$.
{: .prompt-info}

- PROOF

Denote $$X = \begin{bmatrix} x_1 & \cdots & x_n\end{bmatrix}$$, so that $$\mathcal{B} = \{x_1, \dots, x_n\}$$. Then $X$ is the change-of-coordinates matrix $$X_\mathcal{B}$$ where

$$X[x]_\mathcal{B} = x \hspace{0.5cm}\text{and}\hspace{0.5cm} [x]_\mathcal{B} = X^{-1}x$$

For $T(x) = Ax$ in $R^n$:

$$\begin{align}
{\large[} T {\large]}_\mathcal{B} & = {\Large[}\begin{array}{ccc}
	{\large[} T(x_1) {\large]}_\mathcal{B} & \cdots & {\large[} T(x_n) {\large]}_\mathcal{B}
	\end{array}{\Large]} \\
	& = {\Large[}\begin{array}{ccc}
	{\large[} Ax_1 {\large]}_\mathcal{B} & \cdots & {\large[} Ax_n {\large]}_\mathcal{B}
	\end{array}{\Large]} \\
	& = {\Large[}\begin{array}{ccc}
	X^{-1}Ax_1 & \cdots & X^{-1}Ax_n
	\end{array}{\Large]} \\
	& = X^{-1}A\begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix} \\
	& = X^{-1}AX
\end{align}$$

Since $A = X\Lambda X^{-1}$, we have

$${\large[}T{\large]}_\mathcal{B} = X^{-1}AX = \Lambda$$

QED.

Therefore the mappings $x \mapsto Ax$ and $u \mapsto \Lambda u$ describes _the same_ linear transformation, relative to _different bases_.

#### Similarity of matrix representations

The proof above did not use the fact that $\Lambda$ was diagonal. Thus if $A$ is similar to a matrix $C$, where $A = PCP^{-1}$, then $C$ is the $\mathcal{B}$-matrix for the transformation $x \mapsto Ax$ when the basis $\mathcal{B}$ is formed from columns of $P$. 

![similarity](app-P294.png)

Conversely, if $T: R^n \mapsto R^m$ is defined by $T(x) = Ax$, and if $\mathcal{B}$ is any basis of $R^n$, then the $\mathcal{B}$-matrix for $T$ is similar to $A$. 

The calculations in the proof above show that, if $P$ is the matrix whose columns come from the vectors in $\mathcal{B}$, then $${\large[}T{\large]}_\mathcal{B} = P^{-1}AP$$. The set of all matrices similar to $A$ coincides with the set of all matrix representations of $x \mapsto Ax$.

More to note:

> Efficiently computing the $\mathcal{B}$-matrix $P^{-1}AP$:
1. Multiply $AP$.
2. Row reduce: \
	$$\left[\begin{array}{c|c}P & AP\end{array}\right]\longrightarrow
	\left[\begin{array}{c|c}I & P^{-1}AP\end{array}\right]$$
{: .prompt-tip}
